{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2081cce3",
   "metadata": {
    "id": "2081cce3"
   },
   "source": [
    "# Discover the Higgs with Deep Neural Networks\n",
    "# Chapter 2: Create and Train a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fd8810",
   "metadata": {},
   "source": [
    "In this chapter you will train your very first neural network. At the beginning the data preparation of the first chapter is done to enable the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9615674",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f9615674",
    "outputId": "83e88f84-c0ec-4e77-e620-6bf415c816d1"
   },
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from numpy.random import seed\n",
    "import os\n",
    "\n",
    "# Import some common functions created for this notebook\n",
    "import common\n",
    "\n",
    "# Random state\n",
    "random_state = 21\n",
    "_ = np.random.RandomState(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09ff585",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e93ecb",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a2ae28",
   "metadata": {
    "id": "22a2ae28"
   },
   "outputs": [],
   "source": [
    "# Define the input samples\n",
    "sample_list_signal = ['ggH125_ZZ4lep', 'VBFH125_ZZ4lep', 'WH125_ZZ4lep', 'ZH125_ZZ4lep']\n",
    "sample_list_background = ['llll', 'Zee', 'Zmumu', 'ttbar_lep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b2ae67",
   "metadata": {
    "id": "e1b2ae67"
   },
   "outputs": [],
   "source": [
    "sample_path = 'input'\n",
    "# Read all the samples\n",
    "no_selection_data_frames = {}\n",
    "for sample in sample_list_signal + sample_list_background:\n",
    "    no_selection_data_frames[sample] = pd.read_csv(os.path.join(sample_path, sample + '.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7d2bda",
   "metadata": {
    "id": "9a7d2bda"
   },
   "source": [
    "### Event Pre-Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef08fd17",
   "metadata": {},
   "source": [
    "Import the pre-selection functions saved during the first chapter. If the modules are not found solve and execute the notebook of the first chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6824ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.selection_lepton_charge import selection_lepton_charge\n",
    "from functions.selection_lepton_type import selection_lepton_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fc29db",
   "metadata": {
    "id": "13fc29db"
   },
   "outputs": [],
   "source": [
    "# Create a copy of the original data frame to investigate later\n",
    "data_frames = no_selection_data_frames.copy()\n",
    "\n",
    "# Apply the chosen selection criteria\n",
    "for sample in sample_list_signal + sample_list_background:\n",
    "    # Selection on lepton type\n",
    "    type_selection = np.vectorize(selection_lepton_type)(\n",
    "        data_frames[sample].lep1_pdgId,\n",
    "        data_frames[sample].lep2_pdgId,\n",
    "        data_frames[sample].lep3_pdgId,\n",
    "        data_frames[sample].lep4_pdgId)\n",
    "    data_frames[sample] = data_frames[sample][type_selection]\n",
    "\n",
    "    # Selection on lepton charge\n",
    "    charge_selection = np.vectorize(selection_lepton_charge)(\n",
    "        data_frames[sample].lep1_charge,\n",
    "        data_frames[sample].lep2_charge,\n",
    "        data_frames[sample].lep3_charge,\n",
    "        data_frames[sample].lep4_charge)\n",
    "    data_frames[sample] = data_frames[sample][charge_selection]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb2fd69",
   "metadata": {},
   "source": [
    "### Get Test and Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a188c582",
   "metadata": {},
   "source": [
    "During training, the parameters in the neural network are adjusted to the training data. In order to investigate  how the neural network performs on completely unseen data, we will only train on a subset of the given data. 60% of the data will be used for training and 40% for later testing. The split can be done by the `split_data_frames` of the common module. In this addition to the splitting this function rescales the event weights in the given dataframes. So if we split in training and validation data both will have less simulated events than the original dataframes but the same total prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41349ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_frames, test_data_frames = common.split_data_frames(data_frames, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47992227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of simulated events and the total prediction for the full, train, and test datasets\n",
    "for name in data_frames.keys():\n",
    "    print(name)\n",
    "    full_set = data_frames[name]\n",
    "    train_set = train_data_frames[name]\n",
    "    test_set = test_data_frames[name]\n",
    "    full_set_pred = full_set['totalWeight'].sum()\n",
    "    train_set_pred = train_set['totalWeight'].sum()\n",
    "    test_set_pred = test_set['totalWeight'].sum()\n",
    "    \n",
    "    \n",
    "    print(f'Number of simulated events:\\tfull:{len(full_set)}\\ttrain:{len(train_set)}\\ttest:{len(test_set)}')\n",
    "    print(f'Number of predicted events:\\tfull:{round(full_set_pred, 3)}\\ttrain:{round(train_set_pred, 3)}\\ttest:{round(test_set_pred, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6UOuLmyqGkVq",
   "metadata": {
    "id": "6UOuLmyqGkVq"
   },
   "source": [
    "## Create the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YDoe0Nba4osz",
   "metadata": {
    "id": "YDoe0Nba4osz"
   },
   "source": [
    "Lets start with a very simple training on the transverse momentum of the leptons. To speed up the training not all processes are considered for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZLhbQ-gYKIa4",
   "metadata": {
    "id": "ZLhbQ-gYKIa4"
   },
   "outputs": [],
   "source": [
    "# The training input variables\n",
    "training_variables = ['lep1_pt', 'lep2_pt', 'lep3_pt', 'lep4_pt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GkPQ0w0h9CjZ",
   "metadata": {
    "id": "GkPQ0w0h9CjZ"
   },
   "source": [
    "Extract the training variables, the event weights, and the classification of the events. The classification is 0 for background processes and 1 for Higgs events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JBnQRQ1A7U-F",
   "metadata": {
    "id": "JBnQRQ1A7U-F"
   },
   "outputs": [],
   "source": [
    "values, weights, classification = common.get_dnn_input(train_data_frames, training_variables, sample_list_signal, sample_list_background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa10b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show values of 6 random events\n",
    "random_idx = [1841, 11852, 15297, 263217, 278357, 331697]\n",
    "# Training variables\n",
    "print('Training values:')\n",
    "print(values[random_idx])\n",
    "# Event weights\n",
    "print('Event weights:')\n",
    "print(weights[random_idx])\n",
    "# Classification\n",
    "print('Classification:')\n",
    "print(classification[random_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65a9792",
   "metadata": {},
   "source": [
    "Tensorflow is used to create and train neural networks. For this purpose, the required module is imported and the existing training values and classifications are transformed into a Tensorflow dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QnyJWmRnLNSK",
   "metadata": {
    "id": "QnyJWmRnLNSK"
   },
   "outputs": [],
   "source": [
    "# Import the tensorflow module to create a neural network\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S8WnIC0VToQw",
   "metadata": {
    "id": "S8WnIC0VToQw"
   },
   "outputs": [],
   "source": [
    "# Convert the data to tensorflow datasets\n",
    "train_data = Dataset.from_tensor_slices((values, classification))\n",
    "train_data = train_data.shuffle(len(values), seed=random_state)\n",
    "# Set the batch size\n",
    "train_data = train_data.batch(128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257c0909",
   "metadata": {},
   "source": [
    "So lets create a neural network consisting of several layers.\n",
    "\n",
    "A full list of layer types can be found here https://www.tensorflow.org/api_docs/python/tf/keras/layers <br>\n",
    "Some important examples for layers are:\n",
    "- Normalization: Shift and scale the training input to have the center at 0 and a standard deviation of 1 \n",
    "- Dense: a densely connected NN layer\n",
    "- Dropout: randomly sets input to 0. Can decrease overtraining\n",
    "\n",
    "The neurons of each layer are activated by the so called activation function. <br>\n",
    "A full list of provided activation functions can be found here https://www.tensorflow.org/api_docs/python/tf/keras/activations <br>\n",
    "Some examples are:\n",
    "- Linear: $linear(x) = x$\n",
    "- Relu: $relu(x) = max(0, x)$\n",
    "- Exponential: $exponential(x) = e^x$\n",
    "- Sigmoid: $sigmoid(x) = 1 / (1 + e^{-x})$\n",
    "\n",
    "The activation of i-th node in the k-th layer $a_i^{(k)}$ is given by the sum of the activations of the previous layer $a_j^{(k-1)}$ multiplied by the trainings weights $w_{j; i}^{(k)}$ plus a bias $b_i^{(k)}$. For this sum the activation function $f_{activ}$ is called resulting in the activation of the current node.<br>\n",
    "$a_i^{(k)} = f_{activ}(\\sum_j w_{j; i}^{(k)} \\cdot a_j^{k-1} + b_i^{(k)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67b27dd",
   "metadata": {},
   "source": [
    "Let's create following model:\n",
    "<div>\n",
    "<img src='figures/simple_model.png' width='900'/>\n",
    "</div>\n",
    "\n",
    "- The input variables are scaled and shifted to have the mean of 0 and the standard deviation of 1. This normalization improves the convergence of the training process.\n",
    "- Use two dense layers with 60 neurons each. The neurons are activated with the relu function.\n",
    "- To classify background and signal the last layer should only consist of one neuron which has an activation between 0 and 1. The activation between 0 and 1 if given by the sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dba1f2d",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "How many parameters do you expect for this model?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4025b1",
   "metadata": {},
   "source": [
    "Create a normalization layer and adapt it to the training values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_QXqaSb3CYrs",
   "metadata": {
    "id": "_QXqaSb3CYrs"
   },
   "outputs": [],
   "source": [
    "# Normalization layer\n",
    "normalization_layer = tf.keras.layers.Normalization(name='Input_normalization')\n",
    "normalization_layer.adapt(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8074701d",
   "metadata": {},
   "source": [
    "Create a list of model layers and built the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a4MIvWLNn_",
   "metadata": {
    "id": "49a4MIvWLNn_"
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "model_layers = [\n",
    "    normalization_layer,\n",
    "    tf.keras.layers.Dense(60, activation='relu', name='Hidden_layer_1'),\n",
    "    tf.keras.layers.Dense(60, activation='relu', name='Hidden_layer_2'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid', name='Output_layer'),\n",
    "]\n",
    "model = tf.keras.models.Sequential(model_layers, name='simple_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b3d324",
   "metadata": {},
   "source": [
    "Before we start the training of the model lets check the shape and the number of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CJkzGU4giA6_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CJkzGU4giA6_",
    "outputId": "37497ac1-9ecf-49d0-cd5a-cce305d8d1e2"
   },
   "outputs": [],
   "source": [
    "# Display the model's architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ovNG9YVDMD-0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 533
    },
    "id": "ovNG9YVDMD-0",
    "outputId": "fdc7116e-c111-4079-a620-0084d050e06d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True, show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26d232e",
   "metadata": {},
   "source": [
    "## Train the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c3340d",
   "metadata": {},
   "source": [
    "For the training we have to choose an optimization criteria, the loss function. For supervised learning, the loss function returns a value for the divergence between model prediction and the true value. The loss value is high for bad agreement and low for good agreement. Thus, training the model means minimizing the loss function.\n",
    "The seperartion of signal and background is binary classification problem and thus the binary cross-entropy is used to calculate the loss.\n",
    "The binary cross-entropy is given by:<br>\n",
    "$H = -\\frac{1}{N} \\sum_i^N (y_i^{true} log(y_i^{predict}) + (1 - y_i^{true}) log(1 - y_i^{predict}))$\n",
    "\n",
    "As you can see the formula distinguishes signal events with $y_i^{true} = 1$ and background events with $y_i^{true} = 0$. In the following figure you can see the cross entropy for a signal and a background event for different prediction values. The closer the prediction value is to the true classification the smaller the crossentropy is. This is exactly the behavior we need for our loss function, and thus training the neural network means minimizing the mean cross-entropy of all training events.\n",
    "<div>\n",
    "<img src='figures/binary_cross_entropy.png' width='700'/>\n",
    "</div>\n",
    "\n",
    "If you are still wondering why we use the logarithms and why it is called entropy you are encouraged to check out some information theory. The origin of this cross-entropy can be found in the entropy definition in information theory. The entropy of the true distribution is always smaller than the cross-entropy of its estimator. Therefore, minimizing the cross-entropy is equivalent to becomming closer to the true distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qdunQSwiLNzk",
   "metadata": {
    "id": "qdunQSwiLNzk"
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c421ceb",
   "metadata": {},
   "source": [
    "The loss function itself seems not complicated but keep in mind in our example it depends on several thousand parameters of the neural network. Thus, finding the correct parameter combination to minimize the loss is a quite complex problem. The Adam optimizer we will use for the training is a stochastic gradient descent method.\n",
    "\n",
    "When we converted the training a batch size was chosen `train_data = train_data.batch(128)` which are subsets of the given data. While training the optimizer computes the gradient descent und updates the parameters for each batch.\n",
    "After updating the parameters for all batches one after the other, this process can be started again from the beginning. A complete run over all training data is called an epoch. After several epochs, the parameters have been adjusted several times for all training data and hopefully a good combination of neural network parameters has been found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5da3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "adam_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0002, beta_1=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6ca2b4",
   "metadata": {},
   "source": [
    "Configure the model for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K55rSNabMPMb",
   "metadata": {
    "id": "K55rSNabMPMb"
   },
   "outputs": [],
   "source": [
    "# Compilation\n",
    "model.compile(optimizer=adam_optimizer, loss=loss_fn, metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da12bbc",
   "metadata": {},
   "source": [
    "Lets train the model for 5 epochs and store the training history in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0SEg6o8xMPVH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0SEg6o8xMPVH",
    "outputId": "b208f5b1-2bfa-4f96-92fc-c9f5ffa34948",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_data, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978acc56",
   "metadata": {},
   "source": [
    "Lets visualize the training progress of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aTlPvPpMPfD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "2aTlPvPpMPfD",
    "outputId": "265be094-c99d-4a78-b90f-636c38c63e27"
   },
   "outputs": [],
   "source": [
    "# Plot the training history\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "ax.plot(history.history['loss'])\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('loss')\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b0046f",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Do you think the training was done after 5 epochs? \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6387bc",
   "metadata": {},
   "source": [
    "## Save and Load the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37566b9a",
   "metadata": {},
   "source": [
    "As you have seen training a model takes some time. So retrain a model each time you need it is definitely not the way to go. Instead one should save the model after the training and load it for application.\n",
    "\n",
    "Saving and loading a model is very straight forward with:<br>\n",
    "Use `model.save('path/to/location')` to save a model and `model = tf.keras.models.load_model('path/to/location')` to load the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9823abd",
   "metadata": {},
   "source": [
    "Lets save the model you just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b21626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/chapter2_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5713786a",
   "metadata": {},
   "source": [
    "Now load the model and plot its summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40fb9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = tf.keras.models.load_model('models/chapter2_model')\n",
    "\n",
    "saved_model.summary()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
