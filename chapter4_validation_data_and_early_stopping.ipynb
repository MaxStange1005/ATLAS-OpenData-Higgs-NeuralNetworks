{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2081cce3",
   "metadata": {
    "id": "2081cce3"
   },
   "source": [
    "# Discover the Higgs with Deep Neural Networks\n",
    "# Chapter 4: Validation Data and Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5402c353",
   "metadata": {},
   "source": [
    "In this chapter you will create a validation dataset to judge the training progress. To automatically stop the training at right point early stopping will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4424a33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Import the tensorflow module to create a neural network\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "# Import some common functions created for this notebook\n",
    "import common\n",
    "\n",
    "# Random state\n",
    "random_state = 21\n",
    "np.random.seed(random_state)\n",
    "tf.random.set_seed(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02422eb5",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421064c4",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4229b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input samples\n",
    "sample_list_signal = ['ggH125_ZZ4lep', 'VBFH125_ZZ4lep', 'WH125_ZZ4lep', 'ZH125_ZZ4lep']\n",
    "sample_list_background = ['llll', 'Zee', 'Zmumu', 'ttbar_lep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf4fa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = 'input'\n",
    "# Read all the samples\n",
    "no_selection_data_frames = {}\n",
    "for sample in sample_list_signal + sample_list_background:\n",
    "    no_selection_data_frames[sample] = pd.read_csv(os.path.join(sample_path, sample + '.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321c17d1",
   "metadata": {},
   "source": [
    "### Event Pre-Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef6c5e8",
   "metadata": {},
   "source": [
    "Import the pre-selection functions saved during the first chapter. If the modules are not found solve and execute the notebook of the first chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4bb1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.selection_lepton_charge import selection_lepton_charge\n",
    "from functions.selection_lepton_type import selection_lepton_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f105f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original data frame to investigate later\n",
    "data_frames = no_selection_data_frames.copy()\n",
    "\n",
    "# Apply the chosen selection criteria\n",
    "for sample in sample_list_signal + sample_list_background:\n",
    "    # Selection on lepton type\n",
    "    type_selection = np.vectorize(selection_lepton_type)(\n",
    "        data_frames[sample].lep1_pdgId,\n",
    "        data_frames[sample].lep2_pdgId,\n",
    "        data_frames[sample].lep3_pdgId,\n",
    "        data_frames[sample].lep4_pdgId)\n",
    "    data_frames[sample] = data_frames[sample][type_selection]\n",
    "\n",
    "    # Selection on lepton charge\n",
    "    charge_selection = np.vectorize(selection_lepton_charge)(\n",
    "        data_frames[sample].lep1_charge,\n",
    "        data_frames[sample].lep2_charge,\n",
    "        data_frames[sample].lep3_charge,\n",
    "        data_frames[sample].lep4_charge)\n",
    "    data_frames[sample] = data_frames[sample][charge_selection]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5871ed",
   "metadata": {},
   "source": [
    "### Get Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999cf53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to keep 40% for testing\n",
    "train_data_frames, test_data_frames = common.split_data_frames(data_frames, 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfc1160",
   "metadata": {},
   "source": [
    "## Validation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6rodq6-jQWR5",
   "metadata": {
    "id": "6rodq6-jQWR5"
   },
   "source": [
    "How long do we have to train? The right amount of training is very important for the final performance of the network. If the training was too short the model parameters are poorly adapted to the underlying concepts and the model performance is bad. This is called undertraining. If the training was too long the model will start to learn the training data by heart. This overtraining will lead to a very godd performance at the training data but bad performance on unseen data.\n",
    "\n",
    "Thus, to test the performance of the model the model has to be applied on unseen data. After each epoch the model is the model is applied on validation data not used for training. If the classification of the validation data has improved for the current epoch the model performance is still improving. If the performance on the validation data does not improve anymore the training can be stopped.\n",
    "\n",
    "<div>\n",
    "<img src='figures/over_and_under_training.png' width='700'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2f6e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training input variables\n",
    "training_variables = ['lep1_pt', 'lep2_pt', 'lep3_pt', 'lep4_pt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kML3bli6SUfW",
   "metadata": {
    "id": "kML3bli6SUfW"
   },
   "outputs": [],
   "source": [
    "# Import function to split data into train and test data\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KLZpYpYF_Lai",
   "metadata": {
    "id": "KLZpYpYF_Lai"
   },
   "outputs": [],
   "source": [
    "# Extract the values and classification\n",
    "values, _, classification = common.get_dnn_input(train_data_frames, training_variables, sample_list_signal, sample_list_background)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hQb2GTSGSg0o",
   "metadata": {
    "id": "hQb2GTSGSg0o"
   },
   "source": [
    "Use again 2/3 of the training data for the actual training and 1/3 to validate the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5sr3kiRq_Nx8",
   "metadata": {
    "id": "5sr3kiRq_Nx8"
   },
   "outputs": [],
   "source": [
    "# Split into train and validation data\n",
    "train_values, val_values, train_classification, val_classification = train_test_split(values, classification, test_size=1/3, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9fa14b",
   "metadata": {},
   "source": [
    "## Create the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c86e967",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Let's follow the same strategy as before:\n",
    "- Create tensorflow datasets for training and validation data with 128 events per batch\n",
    "- Recreate and adapt the normalization layer\n",
    "- Recreate the tensorflow model with 2 hidden layers and 60 nodes per layer\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PhQihlcESpEO",
   "metadata": {
    "id": "PhQihlcESpEO"
   },
   "outputs": [],
   "source": [
    "# Convert the data to tensorflow datasets\n",
    "train_data = Dataset.from_tensor_slices((train_values, train_classification))\n",
    "train_data = train_data.shuffle(len(train_data), seed=random_state)\n",
    "train_data = train_data.batch(128)\n",
    "val_data = Dataset.from_tensor_slices((val_values, val_classification))\n",
    "val_data = val_data.shuffle(len(val_data), seed=random_state)\n",
    "val_data = val_data.batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zhDgmhgMSCEQ",
   "metadata": {
    "id": "zhDgmhgMSCEQ"
   },
   "outputs": [],
   "source": [
    "# Normalization layer\n",
    "normalization_layer = tf.keras.layers.Normalization()\n",
    "normalization_layer.adapt(train_values)\n",
    "# Create a simple NN\n",
    "model_layers = [\n",
    "    normalization_layer,\n",
    "    tf.keras.layers.Dense(60, activation='relu'),\n",
    "    tf.keras.layers.Dense(60, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "]\n",
    "model = tf.keras.models.Sequential(model_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255f2ae0",
   "metadata": {},
   "source": [
    "## Train the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48498f1d",
   "metadata": {},
   "source": [
    "Lets choose the same loss function and optimizer and compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b1e9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "# Optimizer\n",
    "adam_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0002, beta_1=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ce8494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(optimizer=adam_optimizer, loss=loss_fn, metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fc4cc6",
   "metadata": {},
   "source": [
    "So how to stop the training if the perfomance does not improve anymore?\n",
    "\n",
    "The answer is early stopping. With early stopping you set a value which should be monitored, in our case the loss on the validation data `val_loss`. Since there can be fluctuations in the tested model performance, it is recommended to use a certain patience after which the training should be stopped. If we set `patience=5` the training is stopped if the `va_loss` has not improved for 5 epochs. Since the model performance has potentionally decreased during this 5 epochs set `restore_best_weights=True` to restore the model state with the best performance. The requested number of epochs can be set to a very high number to ensure that the training is only stopped if no improvement was observed anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wyTjuAZHThGI",
   "metadata": {
    "id": "wyTjuAZHThGI"
   },
   "outputs": [],
   "source": [
    "# Early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HpQzXeAgSCZr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HpQzXeAgSCZr",
    "outputId": "aa13403f-a297-40a4-fb1b-450b4004040b"
   },
   "outputs": [],
   "source": [
    "# Train model with early stopping for the validation data performance\n",
    "history = model.fit(train_data, validation_data=val_data, callbacks=[early_stopping], epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8cf6dc",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Plot the training loss (<code>history.history['loss']</code>) and validation loss (<code>history.history['val_loss']</code>) of the training history. Describe what behavior you can observe for each performance trend.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uaCAcUKUWMuh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 543
    },
    "id": "uaCAcUKUWMuh",
    "outputId": "8d15f192-debc-4338-b243-6a2c310b9d16",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the training history\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "# Training loss\n",
    "ax.plot(history.history['loss'], label='training')\n",
    "# Validation loss\n",
    "ax.plot(history.history['val_loss'], label='validation')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('loss')\n",
    "ax.legend()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebce0b3",
   "metadata": {},
   "source": [
    "## Apply and Evaluate the Neural Network on Training and Validation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1693cf8",
   "metadata": {},
   "source": [
    "The model itself has already an implemented evaluation function. When a tensorflow set is provided it returns the loss and accuracy on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-2pKN3BvCd26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-2pKN3BvCd26",
    "outputId": "b91378e3-78d0-411b-9b24-3562da837410",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate model on training\n",
    "model_train_evaluation = model.evaluate(train_data)\n",
    "\n",
    "print(f'train loss = {round(model_train_evaluation[0], 5)}\\ttrain accuracy = {round(model_train_evaluation[1], 5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4bf5d3",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Evaluate the model on validation data and compare the results to the validation on training data.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c754473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on validation data\n",
    "model_val_evaluation = model.evaluate(val_data)\n",
    "\n",
    "print(f'val loss = {round(model_val_evaluation[0], 5)}\\tval accuracy = {round(model_val_evaluation[1], 5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c46efdb",
   "metadata": {},
   "source": [
    "Now lets apply the model on the train and validation data and plot the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Tq0pWJFBWM5x",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tq0pWJFBWM5x",
    "outputId": "d8514d14-7782-421e-9215-5aecfaae62f3"
   },
   "outputs": [],
   "source": [
    "# Apply the model for training and validation values\n",
    "train_prediction = model.predict(train_values)\n",
    "val_prediction = model.predict(val_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1WBms0WNCc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "id": "5d1WBms0WNCc",
    "outputId": "0ee6e8ae-e53d-4472-a3b9-77ae0dcad6b7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the model output\n",
    "common.plot_dnn_output(train_prediction, train_classification, val_prediction, val_classification)\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d4156a",
   "metadata": {},
   "source": [
    "As you can see the classification by the model on traning and validation data is very consistent. This is great :)<br>\n",
    "If we would see a significant difference in train and validation classification this would be a clear sign for overtraining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5ecf25",
   "metadata": {},
   "source": [
    "## Prediction on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abad074e",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Use <code>common.apply_dnn_model(...)</code> to apply the model for all samples in <code>test_data_frames</code> and plot the classification.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bzgiLWMWWGmw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bzgiLWMWWGmw",
    "outputId": "3f8d6b0d-2d86-41d8-c85e-4890f7189156"
   },
   "outputs": [],
   "source": [
    "# Apply the model\n",
    "data_frames_apply_dnn = common.apply_dnn_model(model, data_frames, training_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VgBXHg_6bAP7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "VgBXHg_6bAP7",
    "outputId": "e31607c8-8c43-4b39-8dba-c89c3402acf5"
   },
   "outputs": [],
   "source": [
    "model_prediction = {'variable': 'model_prediction',\n",
    "                    'binning': np.linspace(0, 1, 50),\n",
    "                    'xlabel': 'prediction'}\n",
    "common.plot_hist(model_prediction, data_frames_apply_dnn, show_data=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RRuPU0vhF1Ch",
   "metadata": {
    "id": "RRuPU0vhF1Ch"
   },
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "The $llll$ events are mostly classified as background and the Higgs events tend to the signal classification. However, the classification of the other backgrounds seems mostly random and even a bit shifted towards the signal classification. What could be the reason for this?\n",
    "</font>\n",
    "\n",
    "<font color='green'>\n",
    "Answer:\n",
    "\n",
    "The number of training events for Zee, Zmumu and ttbar_lep is much smaller than for the other samples. Thus, they are hardly considered in the training.\n",
    "    \n",
    "In the first chapter we have observed following number of events in the full dataset:\n",
    "- ggH125_ZZ4lep: 161451\n",
    "- VBFH125_ZZ4lep: 186870\n",
    "- WH125_ZZ4lep: 9772\n",
    "- ZH125_ZZ4lep: 11947\n",
    "- $llll$: 523957\n",
    "- Zee: 243\n",
    "- Zmumu: 257\n",
    "- ttbar_lep: 334\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bada1f",
   "metadata": {},
   "source": [
    "## Save and Load a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97a3a59",
   "metadata": {},
   "source": [
    "Save the model for a later comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7321b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/chapter4_model')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
