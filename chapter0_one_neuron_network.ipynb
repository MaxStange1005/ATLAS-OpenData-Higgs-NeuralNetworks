{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec708eea",
   "metadata": {},
   "source": [
    "# A Neural Network with just one Neuron?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6f3d51-8c0b-4819-b308-d694fef6cb80",
   "metadata": {},
   "source": [
    "In general neural networks represent very complicated functions with a huge amount of trainable parameters. To visualize the training process this chapter will focus on a minimal neural network with just two trainable parameters. This chapter is meant to give a short and easy example of a neural network application. For a deeper understanding of neural networks feel free to follow the next chapters :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e28b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import functools\n",
    "\n",
    "# Import some common functions created for this notebook\n",
    "import common\n",
    "\n",
    "# Set a random state\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3e13d1-2050-4694-ae71-02f6e0f979b7",
   "metadata": {},
   "source": [
    "### Which problem could be solved?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab11294-3fc4-43a5-af9b-a79f81bc3cd7",
   "metadata": {},
   "source": [
    "Since the model we want to train will be extremely simple we also have to find a task that can be solved only by two trainable parameters.\n",
    "In the following we want the neural network to distinguish two sets of points according to their x- and y-coordinate.\n",
    "The distribution of each set of points is determined by a Gaussian distribution.\n",
    "\n",
    "One set of set of points will be centered at $\\mu_1 = (1, 1)$ and the second set is distributed around $\\mu_2 = (-1, -1)$. In order to have a visible separation but still a relevant overlap the covariance matrix for both sets of points is $ cov = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614cf263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for the two distributions\n",
    "mean1 = np.array([-1, -1])\n",
    "mean2 = np.array([1, 1])\n",
    "\n",
    "# Covariance matrix (assuming diagonal covariance matrix for simplicity)\n",
    "cov_matrix = np.eye(2)\n",
    "\n",
    "# Number of points in each set\n",
    "num_points = 1000\n",
    "\n",
    "# Generate points for each distribution\n",
    "points_set1 = common.generate_gaussian_points(mean1, cov_matrix, num_points)\n",
    "points_set2 = common.generate_gaussian_points(mean2, cov_matrix, num_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cce8bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the generated points\n",
    "plt.scatter(points_set1[:, 0], points_set1[:, 1], label='Set 1', alpha=0.3, s=10, c='darkorange')\n",
    "plt.scatter(points_set2[:, 0], points_set2[:, 1], label='Set 2', alpha=0.3, s=10, c='blue')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fb6554-03b3-48fc-9f70-782ee5d93e38",
   "metadata": {},
   "source": [
    "As you can see the two sets of points are overlapping and so a perfect separation will not be possible. Nevertheless, the majority of the two sets differ significantly. Let's see if this difference can be learned by the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815148fc-4418-4dd1-b299-fbcdc777f9bc",
   "metadata": {},
   "source": [
    "Since a network doesn't know of the concept of \"set 1\" and \"set 2\" we assign numeric labels to the points. The points of \"set 1\" get the label 0 and the points of \"set 2\" the label 1. Thus the neural network can be a function classifying a point according to its coordinates by a output score between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05296ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels for the two sets (0 for Set 1, 1 for Set 2)\n",
    "labels_set1 = np.zeros(num_points)\n",
    "labels_set2 = np.ones(num_points)\n",
    "\n",
    "# Combine the data and labels\n",
    "point_coordinates = np.concatenate([points_set1, points_set2], axis=0)\n",
    "point_labels = np.concatenate([labels_set1, labels_set2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ecc240-26f6-4de8-89e6-9e6a90c3ad27",
   "metadata": {},
   "source": [
    "### How to build a network for classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdd4d00-72ce-4abe-a972-59e9a0332303",
   "metadata": {},
   "source": [
    "Now we have the required labeled input data to perform a supervised training of a neural network.\n",
    "In this machine learning course we will use tensorflow which is one of the commmonly used machine learning libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def0faab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014b55bb-e379-4d96-9883-d7c6afcfa1ad",
   "metadata": {},
   "source": [
    "To classify the points our neural network needs an input layer with two input neurons representing the x- and y-coordinate of the given points. Usually, the input layer of a network is followed by several hidden layers but for our example, we will continue directly with the output score\n",
    "<div>\n",
    "<center>\n",
    "<img src='figures/DNN_1_neuron.png' width='400'/>\n",
    "</center>\n",
    "</div>\n",
    "\n",
    "The activation of the score neuron is given by its corresponding activation function applied to the weighted sum of the input neurons.\n",
    "\\begin{equation}\n",
    "score = f_{activation}(w_1 \\cdot x + w_2 \\cdot y)\n",
    "\\end{equation}\n",
    "\n",
    "To enforce a classification between 0 and 1 we use the Sigmoid function for the score activation\n",
    "\\begin{equation}\n",
    "Sigmod(x) = \\frac{1}{1 + e^{-x}}\n",
    "\\end{equation}\n",
    "\n",
    "<div>\n",
    "<center>\n",
    "<img src='figures/sigmoid.png' width='500'/>\n",
    "</center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32f02cb-5fec-464b-ac81-4c4b2de90ae7",
   "metadata": {},
   "source": [
    "Thus the resulting network has two trainable parameters, $w_1$ and $w_2$.\n",
    "\n",
    "We create this network as a sequential keras model with two input parameters. The activation of this one layer is given by the Sigmoid function and we don't use any trainable bias for this layer to keep it as simple as possible.\n",
    "Usually one uses a random initialization of the trainable parameters but in order to investigate the training progress in more detail we initialize the parameters $w_1$ and $w_2$ both to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076e2667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the neural network with bias fixed at zero\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1, input_dim=2, activation='sigmoid', use_bias=False, kernel_initializer=tf.keras.initializers.Constant([0.0, 0.0]))\n",
    "])\n",
    "\n",
    "# Display the model's architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25583233-2a0c-40a0-80c9-c7ce2a9de2f6",
   "metadata": {},
   "source": [
    "### How to train the network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a85842f-614e-423f-b2a2-52532e34d3f4",
   "metadata": {},
   "source": [
    "The training of the network is the variation of the weights $w_1$ and $w_2$ to get the resulting score for each point as close as possible to its actual label. This is achieved by calculating the current score for several points and comparing it with the actual label.\n",
    "\\begin{equation}\n",
    "score(w_1, w_2) = Sigmoid \\left( \n",
    "w_1 \\cdot \\begin{bmatrix} -1.03 \\\\ 0.63 \\\\ -0.36 \\\\ \\vdots \\\\ 0.86 \\\\ 1.47 \\\\ 0.59 \\\\\\end{bmatrix} + w_2 \\cdot \\begin{bmatrix} -1.21 \\\\ -2.15 \\\\ -2.66 \\\\ \\vdots \\\\ 1.74 \\\\ 0.07 \\\\ 1.69 \\\\\\end{bmatrix}\n",
    "\\right)\n",
    "= \\begin{bmatrix} 0.32 \\\\ 0.49 \\\\ 0.12 \\\\ \\vdots \\\\ 0.85 \\\\ 0.69 \\\\ 0.55 \\\\\\end{bmatrix}\n",
    "\\qquad \\xleftrightarrow{\\text{difference} \\,=\\,  loss} \\qquad\n",
    "label = \\begin{bmatrix} \\color{BurntOrange}0.00 \\\\ \\color{BurntOrange}0.00 \\\\ \\color{BurntOrange}0.00 \\\\ \\vdots \\\\ \\color{Blue}1.00 \\\\ \\color{Blue}1.00 \\\\ \\color{Blue}1.00 \\\\\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "For a binary classification, the difference (for machine learning called loss) between the label and score is usually given by the binary cross entropy (BCE). The BCE depends on both the score and the label, and the smaller the loss, the better the agreement between the score and the label.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "loss_\\text{BCE}(score, label) & = \\color{Blue}-label \\cdot log(score) \\color{BurntOrange}\\,-\\,(1-label) \\cdot log(1-score) \\\\\n",
    "& =\n",
    "\\begin{cases}\n",
    "     \\color{BurntOrange} -log(1-score) & \\text{for Set 1 } (label = 0)\\\\\n",
    "    \\color{Blue} -log(score) & \\text{for Set 2 } (label = 1)\n",
    "\\end{cases}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "<div>\n",
    "<center>\n",
    "<img src='figures/binary_cross_entropy_points.png' width='600'/>\n",
    "</center>\n",
    "</div>\n",
    "\n",
    "Since our network only has two trainable parameters, we can directly visualize the mean loss for the given points as a function of $w_1$ and $w_2$.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "loss_\\text{BCE}(w_1, w_2) = & \\frac{1}{N} \\sum_i^N \\left(\\color{Blue}-label_i \\cdot log(score_i(w_1, w_2)) \\color{BurntOrange}\\,-\\,(1-label_i) \\cdot log(1-score_i(w_1, w_2)) \\color{black} \\right) \\\\\n",
    "score_i(w_1, w_2) = & \\, Sigmoid(w_1 \\cdot x_i + w_2 \\cdot y_i)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "<div>\n",
    "<center>\n",
    "<img src='figures/loss_surface_rotation.gif' width='700'/>\n",
    "</center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72333a1e-bae8-4f75-abc8-88a1bc30f62f",
   "metadata": {},
   "source": [
    "As you can see, the mean BCE loss has a clear minimum. At this minimum, the network gives the best classification for the given set of points.\n",
    "During training, the parameters $w_1$ and $w_2$ are now adjusted towards this minimum until there is no more improvement. For the given network, this minimization of the loss seems trivial, but note that usually, networks can have hundreds of thousands if not many more parameters. For this reason, special optimization algorithms are used which efficiently search for the global minima even in very large parameter spaces. With the chosen loss and a suitable optimizer, the network can now be compiled and is ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b21583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "# Optimizer\n",
    "adam_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.005, beta_1=0.7)\n",
    "\n",
    "# Compilation\n",
    "model.compile(optimizer=adam_optimizer, loss=loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1b729f-6abd-4925-911c-f94b98dfec66",
   "metadata": {},
   "source": [
    "During training, the average loss is calculated for the given training data, the direction in which the loss decreases in the parameter space is determined, a small step is taken in this direction and then this process is repeated. However, such a training step is usually not performed on the entire training data set. For training, the training data set is divided into smaller sets, so-called batches, which are then trained on one after the other. On the one hand, this is advantageous if you have a very large set of training data. The division into batches speeds up the training in such a case, as only a fraction of the training data needs to be evaluated at once. On the other hand, the use of batches brings a certain noise into the training. This noise can prevent the training from getting stuck in small local minima, as all batches differ slightly from each other.\n",
    "A complete run over all batches is referred to as an epoch in which the network has seen the entire training data set once. The training can then be continued for any number of further epochs until the minimum of the loss has been reached.\n",
    "\n",
    "To train our neural network for classification we combine the point positions with their label to a tensorflow dataset. To have evenly distributed training data the dataset gets shuffled for the training. Then the dataset gets split into batches of 32 data points each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065b83b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset.from_tensor_slices((point_coordinates, point_labels))\n",
    "train_data = train_data.shuffle(len(point_coordinates), seed=random_state)\n",
    "# Set the batch size\n",
    "train_data = train_data.batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4a3841-7e97-4cf6-844c-8c021f92a5cd",
   "metadata": {},
   "source": [
    "Before we start the training let's define a callback function to store current training parameters $w_1$ and $w_2$ while training. Usually, this is not needed but it will allow us to visualize the training process in more detail later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a92184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store parameter values during training\n",
    "weight1_history = [0]\n",
    "weight2_history = [0]\n",
    "\n",
    "# Custom callback to store weights during training\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        weights = self.model.get_weights()\n",
    "        weights = weights[0].flatten()\n",
    "        weight1_history.append(weights[0])\n",
    "        weight2_history.append(weights[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae50e732-8d3b-4329-bc15-308b32928d90",
   "metadata": {},
   "source": [
    "Finally we can train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a5eff8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_history = model.fit(train_data, epochs=50, batch_size=32, callbacks=[CustomCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2c5891-078f-400f-8fc3-592f24663939",
   "metadata": {},
   "source": [
    "After training, the can be used to give predictions for the given data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34b8711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels for the points\n",
    "point_predictions = model.predict(point_coordinates)\n",
    "\n",
    "\n",
    "# Select some random points\n",
    "idx = [0, 400, 800, 1200, 1600]\n",
    "\n",
    "print(f'The true labels: {point_labels[idx]}')\n",
    "print(f'The classification: {point_predictions[idx].flatten()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affbb4a5-ec51-4cf3-939a-7e8ab5456d19",
   "metadata": {},
   "source": [
    "Let's visualize the classification of the data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7cf278-7391-465b-ae82-c0da6f0d092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model output as a histogram\n",
    "common.plot_dnn_output(point_predictions, point_labels)\n",
    "\n",
    "# Visualize the classification in a scatter plot\n",
    "common.classification_scatter_plot(point_coordinates, point_predictions)\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f59a15-4701-4ea1-8c7f-ea84455dc45d",
   "metadata": {},
   "source": [
    "To better understand the training process a interactive plot is given in the following. You can choose the training epoch you are interested in to see the current training parameters and classification performance of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd99ef9-4b2d-4575-a94c-8f64d07dc09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "%matplotlib widget\n",
    "\n",
    "# Give the points and the training history to the visualization function\n",
    "partial_visualise = functools.partial(\n",
    "    common.visualise_training_minimal_dnn,\n",
    "    weight1_history=weight1_history,\n",
    "    weight2_history=weight2_history,\n",
    "    point_coordinates=point_coordinates,\n",
    "    point_labels=point_labels\n",
    ")\n",
    "\n",
    "# Define a wrapper function that calls the partial function\n",
    "def interactive_plot_wrapper(epoch):\n",
    "    return partial_visualise(epoch=epoch)\n",
    "\n",
    "# Interactive plot\n",
    "_ = interact(interactive_plot_wrapper, epoch=(1, 50, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
