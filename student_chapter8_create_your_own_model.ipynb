{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2081cce3",
   "metadata": {
    "id": "2081cce3"
   },
   "source": [
    "# Discover the Higgs with Deep Neural Networks\n",
    "# Chapter 8: Your Own Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78f07e1",
   "metadata": {},
   "source": [
    "Now it is your turn to create your own model to hunt for the Higgs boson. For the beginning, we will have a look on a baseline model. This baseline model does already a good job in the classification and it is up to you to create a better one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244f5820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from numpy.random import seed\n",
    "import os\n",
    "\n",
    "# Import the tensorflow module to create a neural network\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "# Import function to split data into train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import the kFold module for cross-validation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Import some common functions created for this notebook\n",
    "import common\n",
    "\n",
    "# Random state\n",
    "random_state = 21\n",
    "_ = np.random.RandomState(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600a4cea",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b61d26",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15227a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input samples\n",
    "sample_list_signal = ['ggH125_ZZ4lep', 'VBFH125_ZZ4lep', 'WH125_ZZ4lep', 'ZH125_ZZ4lep']\n",
    "sample_list_background = ['llll', 'Zee', 'Zmumu', 'ttbar_lep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f49139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = 'input'\n",
    "# Read all the samples\n",
    "no_selection_data_frames = {}\n",
    "for sample in sample_list_signal + sample_list_background:\n",
    "    no_selection_data_frames[sample] = pd.read_csv(os.path.join(sample_path, sample + '.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd182701",
   "metadata": {},
   "source": [
    "### Event Pre-Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a45fe8a",
   "metadata": {},
   "source": [
    "Import the pre-selection functions saved during the first chapter. If the modules are not found solve and execute the notebook of the first chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be0e5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.selection_lepton_charge import selection_lepton_charge\n",
    "from functions.selection_lepton_type import selection_lepton_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae94f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original data frame to investigate later\n",
    "data_frames = no_selection_data_frames.copy()\n",
    "\n",
    "# Apply the chosen selection criteria\n",
    "for sample in sample_list_signal + sample_list_background:\n",
    "    # Selection on lepton type\n",
    "    type_selection = np.vectorize(selection_lepton_type)(\n",
    "        data_frames[sample].lep1_pdgId,\n",
    "        data_frames[sample].lep2_pdgId,\n",
    "        data_frames[sample].lep3_pdgId,\n",
    "        data_frames[sample].lep4_pdgId)\n",
    "    data_frames[sample] = data_frames[sample][type_selection]\n",
    "\n",
    "    # Selection on lepton charge\n",
    "    charge_selection = np.vectorize(selection_lepton_charge)(\n",
    "        data_frames[sample].lep1_charge,\n",
    "        data_frames[sample].lep2_charge,\n",
    "        data_frames[sample].lep3_charge,\n",
    "        data_frames[sample].lep4_charge)\n",
    "    data_frames[sample] = data_frames[sample][charge_selection]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b6b473",
   "metadata": {},
   "source": [
    "### Get Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f803dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to keep 40% for testing\n",
    "train_data_frames, test_data_frames = common.split_data_frames(data_frames, 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48f6205",
   "metadata": {},
   "source": [
    "Import the reweighting function to train with event weights. If the module is not found solve and execute the notebook of chapter 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0303d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.reweight_weights import reweight_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eb76d6",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6389f87a",
   "metadata": {},
   "source": [
    "Lets load the baseline models. The baseline model was trained with cross validation with the same split as in chapter 6.\n",
    "\n",
    "In the cross validation the baseline model resulted in the validation loss:<br>\n",
    "`[0.25824, 0.27069, 0.23760]`\n",
    "\n",
    "Thus the val loss of the model is $0.256 \\pm 0.014$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae461db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_models = []\n",
    "for idx in range(3):\n",
    "    print(f'baseline_models/model_crossval{idx}')\n",
    "    model = tf.keras.models.load_model(f'baseline_models/model_crossval{idx}')\n",
    "    baseline_models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f16e892",
   "metadata": {},
   "source": [
    "## Create the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27191cd9",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "1. Use reweighted event weights for your training\n",
    "2. Choose a setup for your model\n",
    "3. Train your model with early stopping\n",
    "4. Plot the training history and binary classification on training and validation data\n",
    "5. check your validation loss <br>\n",
    "    If the validation loss is not better than for the baseline model by two standard deviations of the baseline validation loss: go back to step 2.\n",
    "6. Validate your results with cross validation and calculate the mean validation loss and its standard deviation\n",
    "    If the loss is not significantly better than for the baseline model: go back to step 2.\n",
    "7. Save your models and training plots\n",
    "8. Plot the training history\n",
    "\n",
    "Document what you can observe for your own model.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30c159a",
   "metadata": {},
   "source": [
    "Train your model on all <u><b>low level</b></u> variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef382ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training input variables\n",
    "training_variables = ['lep1_pt', 'lep2_pt', 'lep3_pt', 'lep4_pt']\n",
    "training_variables += ['lep1_e', 'lep2_e', 'lep3_e', 'lep4_e']\n",
    "training_variables += ['lep1_charge', 'lep2_charge', 'lep3_charge', 'lep4_charge']\n",
    "training_variables += ['lep1_pdgId', 'lep2_pdgId', 'lep3_pdgId', 'lep4_pdgId']\n",
    "training_variables += ['lep1_phi', 'lep2_phi', 'lep3_phi', 'lep4_phi']\n",
    "training_variables += ['lep1_eta', 'lep2_eta', 'lep3_eta', 'lep4_eta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ef18be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the values, weights, and classification of the data\n",
    "values, weights, classification = common.get_dnn_input(train_data_frames, training_variables, sample_list_signal, sample_list_background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b931e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation data\n",
    "train_values, val_values, train_classification, val_classification = \n",
    "train_weights, val_weights = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4cee92",
   "metadata": {},
   "source": [
    "If you want you can play around with the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3e2930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get reweighted weights\n",
    "\n",
    "\n",
    "# Convert the data to tensorflow datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35a3d25",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Try different neural network shapes with different number of hidden layers and nodes per layer. The number of nodes can also differ from layer to layer.\n",
    "    \n",
    "Note down all the networks you have trained and their corresponding validation loss.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1baf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization layer\n",
    "\n",
    "# Create a simple NN\n",
    "model_layers = [\n",
    "]\n",
    "model = tf.keras.models.Sequential(model_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96132367",
   "metadata": {},
   "source": [
    "## Train the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d7d65d",
   "metadata": {},
   "source": [
    "You can change the learning rate of the optimizer to improve your training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dce6d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "loss_fn = \n",
    "# Optimizer\n",
    "adam_optimizer = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9a1303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model now with the weighted metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c2ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping\n",
    "early_stopping = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5286b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb46dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training history\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "ax.plot(history.history['loss'], label='training')\n",
    "ax.plot(history.history['val_loss'], label='validation')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('loss')\n",
    "ax.legend()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5d1b56",
   "metadata": {},
   "source": [
    "## Apply and Evaluate the Neural Network on Training and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906011f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model for training and validation values\n",
    "train_prediction = \n",
    "val_prediction = \n",
    "# Plot the model output\n",
    "common.plot_dnn_output(train_prediction, train_classification, val_prediction, val_classification)\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce097fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on training and validation data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64da4add",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Is the validation loss by two standard deviations lower than the validation loss of the baseline model?\n",
    "If so save your model and continue with the cross-validation to proof your model setup is better.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1db61c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your model\n",
    "model.save(f'models/chapter8_own_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4374f4f",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5183ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the K-fold Cross Validator\n",
    "kfold = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efce845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the models and their training history\n",
    "kfold_history = []\n",
    "kfold_model = []\n",
    "# Store the evaluation on training and validation data\n",
    "kfold_train_eval_loss = []\n",
    "kfold_train_eval_acc = []\n",
    "kfold_val_eval_loss = []\n",
    "kfold_val_eval_acc = []\n",
    "split_idx = 1\n",
    "for train_indices, val_indices in kfold.split(values):\n",
    "    print(f'Use fold {split_idx}')\n",
    "    split_idx += 1\n",
    "    # Get train and validation data \n",
    "    train_values = values[train_indices]\n",
    "    train_classification = classification[train_indices]\n",
    "    train_weights = weights[train_indices]\n",
    "    val_values = values[val_indices]\n",
    "    val_classification = classification[val_indices]\n",
    "    val_weights = weights[val_indices]\n",
    "    # Get reweighted weights\n",
    "    train_weights_reweighted = \n",
    "    val_weights_reweighted = \n",
    "    # Get train and validation datasets\n",
    "\n",
    "\n",
    "    # Normalization layer\n",
    "\n",
    "    # Create a simple NN\n",
    "    model_layers = [\n",
    "    ]\n",
    "    model = tf.keras.models.Sequential(model_layers)\n",
    "    # Compile model\n",
    "\n",
    "\n",
    "    # Train model\n",
    "    history = \n",
    "\n",
    "    # Append to list\n",
    "    kfold_history.append(history)\n",
    "    kfold_model.append(model)\n",
    "\n",
    "    # Evaluate model on training and validation data\n",
    "    model_train_evaluation = \n",
    "    model_val_evaluation = \n",
    "    kfold_train_eval_loss.append(model_train_evaluation[0])\n",
    "    kfold_train_eval_acc.append(model_train_evaluation[1])\n",
    "    kfold_val_eval_loss.append(model_val_evaluation[0])\n",
    "    kfold_val_eval_acc.append(model_val_evaluation[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bbca51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training history\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('loss')\n",
    "ax.legend()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147e5843",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_mean = \n",
    "val_loss_std = \n",
    "print(f'The val loss of the model is {round(val_loss_mean, 3)} +- {round(val_loss_std, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d1c63b",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Now perform a t-test between your validation losses and the ones of the baseline model. If the p-value is lower than 5% you have proven that your model is indeed better than the baseline one and you can continue with the Higgs search.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf23c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bb3cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The validation losses of the baseline model\n",
    "basline_val_values = [0.25824, 0.27069, 0.23760]\n",
    "\n",
    "# Perform t-test\n",
    "t_stat, p_value = stats.ttest_ind(basline_val_values, kfold_val_eval_loss)\n",
    "\n",
    "print(f'The p-value of the two models having the same performance is {p_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91becdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all cross-validation models\n",
    "for idx, model in enumerate(kfold_model):\n",
    "    # Save the model\n",
    "    model.save(f'models/chapter8_own_model_crossval{idx}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6828bf78",
   "metadata": {},
   "source": [
    "## Higgs Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840825b2",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Load your model created before the cross-validation. Apply the model on the test data to get an prediction for unseen data. Compare this prediction with the ones we had so far in the chapters before.\n",
    "    \n",
    "Calculate the significances you can get by this model for different cut values and compare this to the results in chapter 7.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54492d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "own_model = tf.keras.models.load_model('models/chapter8_own_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfae44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model on test data\n",
    "data_frames_apply_dnn = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88714ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prediction = {'variable': 'model_prediction',\n",
    "                    'binning': np.linspace(0., 1, 20),\n",
    "                    'xlabel': 'prediction'}\n",
    "common.plot_hist(model_prediction, data_frames_apply_dnn, show_data=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb82334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the values, weights, and classification of the test dataset\n",
    "test_values, test_weights, test_classification = common.get_dnn_input(test_data_frames, training_variables, sample_list_signal, sample_list_background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e035e7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data in signal and background\n",
    "test_signal_values = \n",
    "test_signal_weights = \n",
    "test_bkg_values = \n",
    "test_bkg_weights = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35f16b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.get_significances import get_significances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15776038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the baseline model for comparison\n",
    "model_baseline = tf.keras.models.load_model(f'baseline_models/model_crossval0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5b3afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the significances\n",
    "model_baseline_cut_values, model_baseline_significances = \n",
    "own_model_cut_values, own_model_significances = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df31c24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the significances of baseline and own model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fd3d64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
