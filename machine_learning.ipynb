{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2081cce3",
   "metadata": {},
   "source": [
    "# Discover the Higgs with Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060ec659",
   "metadata": {},
   "source": [
    "The input data was created from 13 TeV ATLAS open data downloaded from http://opendata.atlas.cern/release/2020/documentation/index.html\n",
    "\n",
    "For more information read:<br>\n",
    "Review of the 13 TeV ATLAS Open Data release, Techn. Ber., All figures including auxiliary figures are available at https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PUBNOTES/ATL-OREACH-PUB-2020-001: CERN, 2020, url: http://cds.cern.ch/record/2707171"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e312205",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5b2349",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9615674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from numpy.random import seed\n",
    "import common"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3357ffaf",
   "metadata": {},
   "source": [
    "The goal of this lab course is to train a deep neural network to separate Higgs boson signal from background events. The most important signal sample ggH125_ZZ4lep corresponds to the process gg->H->ZZ. The dominant background sample is llll resulting from Z and ZZ decays.\n",
    "After training the DNN model will be used to classify the events of the data samples.\n",
    "\n",
    "Higgs signal samples:\n",
    "- ggH125_ZZ4lep\n",
    "- VBFH125_ZZ4lep\n",
    "- WH125_ZZ4lep\n",
    "- ZH125_ZZ4lep\n",
    "\n",
    "Background samples:\n",
    "- llll\n",
    "- Zee\n",
    "- Zmumu\n",
    "- ttbar_lep\n",
    "\n",
    "Data samples:\n",
    "- data_A\n",
    "- data_B\n",
    "- data_C\n",
    "- data_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a2ae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input samples\n",
    "sample_list_signal = ['ggH125_ZZ4lep']\n",
    "sample_list_background = ['llll']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b2ae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the samples\n",
    "data_frames = {}\n",
    "for sample in sample_list_signal + sample_list_background:\n",
    "    data_frames[sample] = pd.read_csv('input/' + sample + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a1549c",
   "metadata": {},
   "source": [
    "### Input Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe06218b",
   "metadata": {},
   "source": [
    "The input provides several variables to classify the events. Since each event has multiple leptons, they were ordered in descending order based on their transverse momentum. Thus, lepton 1 has the highest transverse momentum, lepton 2 the second highest, and so on. <br>\n",
    "Most of the given variables can be called low-level, because they represent event or object properties, which can be derived directly from the reconstruction in the detector. In contrast to this are high-level variables, which result from the combination of several low-level variables. In the given dataset the only high-level variables are invariant masses of multiple particles:<br>\n",
    "$m_{inv} = \\sqrt{\\left(\\sum\\limits_{i=1}^{n} E_i\\right)^2 - \\left(\\sum\\limits_{i=1}^{n} \\vec{p}_i\\right)^2}$\n",
    "\n",
    "\n",
    "List of all available variables:<br>\n",
    "- Event number\n",
    " - Each simulated event has its own specific number. (not used for training)\n",
    " - Variable name: eventNumber\n",
    "- Event weight\n",
    " - To combine simulated events and finally compare them to data each event has to be scaled by its respective event weight. (not used for training)\n",
    " - Variable name: weight\n",
    "- Number of jets\n",
    " - Jets are particle showers which result primarily from quarks and gluons\n",
    " - Variable name: jet_n\n",
    "- Invariant four lepton mass\n",
    " - The invariant mass $m_{inv}(l_1, l_2, l_3, l_4)$ of the four leptons is extremly sensitive to Higgs boson events. This variable is to be displayed later and thus not used for training\n",
    " - Variable name: lep_m_llll\n",
    "- Invariant two lepton mass\n",
    " - Invariant masses $m_{inv}(l_i, l_j)$ of all combinations of two leptons\n",
    " - Variable names: lep_m_ll_12, lep_m_ll_13, lep_m_ll_14, lep_m_ll_23, lep_m_ll_24, lep_m_ll_34\n",
    "- Transverse momentum of the leptons\n",
    " - The momentum in the plane transverse to the beam axis\n",
    " - Variable names: lep1_pt, lep2_pt, lep3_pt, lep4_pt\n",
    "- Lepton azimuthal angle\n",
    " - The azimuthal angle $\\phi$ is measured in the plane transverse to the beam axis\n",
    " - Variable name: lep1_phi, lep2_phi, lep3_phi, lep4_phi\n",
    "- Lepton pseudo rapidity\n",
    " - The angle $\\theta$ is measured between the lepton track and the beam axis. Since this angle is not invariant against boosts along the beam axis, the pseudo rapidity $\\eta = - \\ln{\\tan{\\frac{\\theta}{2}}}$ is primarily used in the ATLAS analyses\n",
    " - Variable names: lep1_eta, lep2_eta, lep3_eta, lep4_eta\n",
    "- Lepton energy\n",
    " - The energy of the leptons reconstructed from the calorimeter entries\n",
    " - Variable name:lep1_e, lep2_e, lep3_e, lep4_e\n",
    "- Lepton PDG-ID\n",
    " - The lepton type is classified by a n umber given by the Particle-Data-Group. The lepton types are $pdg-id(e)=11$, $pdg-id(\\mu)=13$ and $pdg-id(\\tau)=15$\n",
    " - Variable name: lep1_pdgId, lep2_pdgId, lep3_pdgId, lep4_pdgId\n",
    "- Lepton charge\n",
    " - The charge of the given lepton reconstructed by the lepton track\n",
    " - Variable name: lep1_charge, lep2_charge, lep3_charge, lep4_charge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7d2bda",
   "metadata": {},
   "source": [
    "### Event Pre-Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facd6d51",
   "metadata": {},
   "source": [
    "Although the final selection of the data is to be performed on the basis of a DNN, a rough pre-selection of the data is still useful.\n",
    "For this purpose, selection criteria are defined, which return either true or false based on the event kinematics and thus decide whether the respective event is kept or discarded.\n",
    "Suitable criteria for this analysis are very basic selections that must be clearly fulfilled by H->ZZ->llll processes.\n",
    "\n",
    "\n",
    "Hint: What lepton types and charges are expected in the final state?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa57853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_lep_type(lep_type_0, lep_type_1, lep_type_2, lep_type_3):\n",
    "    # Only keep events like eeee, mumumumu or eemumu\n",
    "    sum_lep_type = lep_type_0 + lep_type_1 + lep_type_2 + lep_type_3\n",
    "    return sum_lep_type == 44 or sum_lep_type == 48 or sum_lep_type == 52\n",
    "\n",
    "\n",
    "def cut_lep_charge(lep_charge_0, lep_charge_1, lep_charge_2, lep_charge_3):\n",
    "    # Only keep events where the sum of all lepton charges is zero\n",
    "    sum_lep_charge = lep_charge_0 + lep_charge_1 + lep_charge_2 + lep_charge_3\n",
    "    return sum_lep_charge == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fc29db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the chosen selection criteria\n",
    "for sample in sample_list_signal + sample_list_background:\n",
    "    # Cut on lepton type\n",
    "    data_frames[sample] = data_frames[sample][np.vectorize(cut_lep_type)(\n",
    "        data_frames[sample].lep1_pdgId,\n",
    "        data_frames[sample].lep2_pdgId,\n",
    "        data_frames[sample].lep3_pdgId,\n",
    "        data_frames[sample].lep4_pdgId)]\n",
    "\n",
    "    # Cut on lepton charge\n",
    "    data_frames[sample] = data_frames[sample][np.vectorize(cut_lep_charge)(\n",
    "        data_frames[sample].lep1_charge,\n",
    "        data_frames[sample].lep2_charge,\n",
    "        data_frames[sample].lep3_charge,\n",
    "        data_frames[sample].lep4_charge)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94202a44",
   "metadata": {},
   "source": [
    "### Data Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e11cc8e",
   "metadata": {},
   "source": [
    "Before one can decide which variables are suitable for training, one must first get a feel for the input variables.\n",
    "For this purpose, the input samples are merged into a set of signal events and a set of background events. Afterwards, the behavior of signal and background can be studied in multiple variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591b7e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the signal and background data frames\n",
    "def merge_data_frames(sample_list, data_frames_dic):\n",
    "    for sample in sample_list:\n",
    "        if sample == sample_list[0]:\n",
    "            output_data_frame = data_frames_dic[sample]\n",
    "        else:\n",
    "            output_data_frame = pd.contact(output_data_frame, data_frames_dic[sample])\n",
    "    return output_data_frame\n",
    "\n",
    "data_frame_signal = merge_data_frames(sample_list_signal, data_frames)\n",
    "data_frame_background = merge_data_frames(sample_list_background, data_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2906682a",
   "metadata": {},
   "source": [
    "The function common.plot_hist(variable, data_frame_1, data_frame_2) plots the given variable of the two data sets.\n",
    "The variable must be a dictionary containing atleast the variable to plot. Additionally one can also specify the binning (list or numpy array) and the xlabel. The created lots are automatically saved in the plots directory<br>\n",
    "An example for the transverse momnetum of the leading lepton is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae8494b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow bins: None!!!!\n",
    "\n",
    "var_lep1_pt = {'variable': 'lep1_pt',\n",
    "               'binning': np.linspace(0, 200, 100),\n",
    "               'xlabel': '$lep_{pt}$[1] [GeV]'}\n",
    "\n",
    "common.plot_hist(var_lep1_pt, data_frame_signal, data_frame_background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff489c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting of all distributions summed!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7415e53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is not correct!!!!!\n",
    "for sample in sample_list_signal + sample_list_background:\n",
    "    print(sample, np.mean(data_frames[sample].weight), len(data_frames[sample].weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74ea3bc",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0e9914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training input variables\n",
    "input_variable_list = ['lep1_pt', 'lep2_pt', 'lep3_pt', 'lep4_pt']\n",
    "input_variable_list += ['lep1_charge', 'lep2_charge', 'lep3_charge', 'lep4_charge']\n",
    "input_variable_list += ['lep1_pdgId', 'lep2_pdgId', 'lep3_pdgId', 'lep4_pdgId']\n",
    "input_variable_list += ['lep1_phi', 'lep2_phi', 'lep3_phi', 'lep4_phi']\n",
    "input_variable_list += ['lep1_eta', 'lep2_eta', 'lep3_eta', 'lep4_eta']\n",
    "#input_variable_list = ['lep_m_ll_12', 'lep_m_ll_13', 'lep_m_ll_14', 'lep_m_ll_23', 'lep_m_ll_24', 'lep_m_ll_34']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a528eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training input\n",
    "input_values = []\n",
    "input_classification = []\n",
    "for sample in sample_list_signal + sample_list_background:\n",
    "    # Classify signal and background (and skip if data)\n",
    "    if sample in sample_list_signal:\n",
    "        # 1 if signal\n",
    "        input_classification.append(np.ones(len(data_frames[sample])))\n",
    "    elif sample in sample_list_background:\n",
    "        # 0 if background\n",
    "        input_classification.append(np.zeros(len(data_frames[sample])))\n",
    "    else:\n",
    "        continue\n",
    "    input_values.append(data_frames[sample][input_variable_list])\n",
    "\n",
    "# Merge the input\n",
    "input_values = np.concatenate(input_values)\n",
    "input_classification = np.concatenate(input_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549adf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=420)\n",
    "\n",
    "def split_indices(df, train_frac, test_frac, val_frac):\n",
    "    #assert train_frac + val_frac + test_frac <= 1\n",
    "    random_values = np.random.rand(len(df))\n",
    "    range1 = random_values <= train_frac\n",
    "    range2 = random_values <= train_frac + test_frac\n",
    "    range3 = random_values <= train_frac + test_frac + val_frac\n",
    "    train_index = range1\n",
    "    test_index = ~range1 & range2\n",
    "    val_index = ~range2 & range3\n",
    "    return train_index, test_index, val_index\n",
    "\n",
    "train_index, test_index, val_index = split_indices(input_values, 0.8, 0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4c6ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_values = input_values[train_index]\n",
    "train_input_classification = input_classification[train_index]\n",
    "test_input_values = input_values[test_index]\n",
    "test_input_classification = input_classification[test_index]\n",
    "val_input_values = input_values[val_index]\n",
    "val_input_classification = input_classification[val_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a5368a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241de309",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  #tf.keras.layers.Flatten(input_shape=[len(input_variable_list)]),\n",
    "  tf.keras.layers.Dense(40, activation='relu'),\n",
    "  tf.keras.layers.Dense(30, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='relu'),\n",
    "  #tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682ad8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(train_input_values).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077a6f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.nn.softmax(predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe2b485",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1747d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_fn(train_input_classification, predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef560b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bcb36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_input_values, train_input_classification, epochs=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be8af87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_input_values,  test_input_classification, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0545f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = model.predict(test_input_values)\n",
    "print(test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a5d0e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
