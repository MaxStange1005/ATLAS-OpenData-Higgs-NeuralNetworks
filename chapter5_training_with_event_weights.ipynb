{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2081cce3",
   "metadata": {
    "id": "2081cce3"
   },
   "source": [
    "# Discover the Higgs with Deep Neural Networks\n",
    "# Chapter 5: Training with Event Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a910c78",
   "metadata": {},
   "source": [
    "In this chapter you will apply event weights for the training. If the weights are applied in the correct way they can significantly improve the training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb67843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from numpy.random import seed\n",
    "import os\n",
    "\n",
    "# Import the tensorflow module to create a neural network\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "# Import function to split data into train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import some common functions created for this notebook\n",
    "import common\n",
    "\n",
    "# Random state\n",
    "random_state = 21\n",
    "_ = np.random.RandomState(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08175d1e",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c850a1",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d385d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input samples\n",
    "sample_list_signal = ['ggH125_ZZ4lep', 'VBFH125_ZZ4lep', 'WH125_ZZ4lep', 'ZH125_ZZ4lep']\n",
    "sample_list_background = ['llll', 'Zee', 'Zmumu', 'ttbar_lep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c776b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = 'input'\n",
    "# Read all the samples\n",
    "no_selection_data_frames = {}\n",
    "for sample in sample_list_signal + sample_list_background:\n",
    "    no_selection_data_frames[sample] = pd.read_csv(os.path.join(sample_path, sample + '.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c267b523",
   "metadata": {},
   "source": [
    "### Event Pre-Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd12321c",
   "metadata": {},
   "source": [
    "Import the pre-selection functions saved during the first chapter. If the modules are not found solve and execute the notebook of the first chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830766b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.selection_lepton_charge import selection_lepton_charge\n",
    "from functions.selection_lepton_type import selection_lepton_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75231f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original data frame to investigate later\n",
    "data_frames = no_selection_data_frames.copy()\n",
    "\n",
    "# Apply the chosen selection criteria\n",
    "for sample in sample_list_signal + sample_list_background:\n",
    "    # Selection on lepton type\n",
    "    type_selection = np.vectorize(selection_lepton_type)(\n",
    "        data_frames[sample].lep1_pdgId,\n",
    "        data_frames[sample].lep2_pdgId,\n",
    "        data_frames[sample].lep3_pdgId,\n",
    "        data_frames[sample].lep4_pdgId)\n",
    "    data_frames[sample] = data_frames[sample][type_selection]\n",
    "\n",
    "    # Selection on lepton charge\n",
    "    charge_selection = np.vectorize(selection_lepton_charge)(\n",
    "        data_frames[sample].lep1_charge,\n",
    "        data_frames[sample].lep2_charge,\n",
    "        data_frames[sample].lep3_charge,\n",
    "        data_frames[sample].lep4_charge)\n",
    "    data_frames[sample] = data_frames[sample][charge_selection]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e9f4ec",
   "metadata": {},
   "source": [
    "### Get Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa0eaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to keep 40% for testing\n",
    "train_data_frames, test_data_frames = common.split_data_frames(data_frames, 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "txeJepxIIPYh",
   "metadata": {
    "id": "txeJepxIIPYh"
   },
   "source": [
    "## Event Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3476c91a",
   "metadata": {},
   "source": [
    "As already explained, significantly more events were simulated than are actually to be expected in the measurement. In addition, the events were generated for different impacts on the final prediction. To account for this, weights are applied to the events to adjust their impact on the prediction. The effect of these event weights can be seen in the following histograms.\n",
    "\n",
    "<center>Prediction without applying the event weights:</center>\n",
    "<div>\n",
    "<img src='figures/event_weights_all_not_applied.png' width='500'/>\n",
    "</div>\n",
    "\n",
    "\n",
    "<center>Prediction when applying the event weights:</center>\n",
    "<div>\n",
    "<img src='figures/event_weights_all_applied.png' width='500'/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46db027e",
   "metadata": {},
   "source": [
    "The simulated events are multiplied by weight factors for the final prediction of measured data. Thus, events with large weights are more important for the prediction than events with small weights. In order to take this into account for the training, the event weights $w_i$ can be included in the calculation of the loss:<br>\n",
    "$H = -\\frac{1}{N} \\sum_i^N w_i (y_i^{true} log(y_i^{predict}) + (1 - y_i^{true}) log(1 - y_i^{predict}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfd1359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training input variables\n",
    "training_variables = ['lep1_pt', 'lep2_pt', 'lep3_pt', 'lep4_pt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30b8ee4",
   "metadata": {},
   "source": [
    "Extract now also the weights for the tarining and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5838211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the values, weights, and classification of the data\n",
    "values, weights, classification = common.get_dnn_input(train_data_frames, training_variables, sample_list_signal, sample_list_background)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a6d138",
   "metadata": {},
   "source": [
    "In order to also split the weights into a training and validation set the same `train_test_split()` can be used. It is very important that the same split is applied for the weights as for the values and classifications. Otherwise, the weights would no longer be associated with the matching events. This can be achieved by using the same random state for the split of values and classification and the split of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771bfcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation data\n",
    "train_values, val_values, train_classification, val_classification = train_test_split(values, classification, test_size=1/3, random_state=random_state)\n",
    "train_weights, val_weights = train_test_split(weights, classification, test_size=1/3, random_state=random_state)[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7346ed43",
   "metadata": {},
   "source": [
    "## Create the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2505d7a6",
   "metadata": {},
   "source": [
    "The use of training weights is very straight forward since they can be directly included in the tensorflow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jM5nQz3dsN3t",
   "metadata": {
    "id": "jM5nQz3dsN3t"
   },
   "outputs": [],
   "source": [
    "# Convert the data to tensorflow datasets\n",
    "train_data = Dataset.from_tensor_slices((train_values, train_classification, train_weights))\n",
    "train_data = train_data.shuffle(len(train_data), seed=random_state)\n",
    "train_data = train_data.batch(128)\n",
    "val_data = Dataset.from_tensor_slices((val_values, val_classification, val_weights))\n",
    "val_data = val_data.shuffle(len(val_data), seed=random_state)\n",
    "val_data = val_data.batch(128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7d5a3c",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Let's follow the same strategy as before:\n",
    "- Recreate and adapt the normalization layer\n",
    "- Recreate the tensorflow model with 2 hidden layers and 60 nodes per layer\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada5KISkbAah",
   "metadata": {
    "id": "ada5KISkbAah"
   },
   "outputs": [],
   "source": [
    "# Normalization layer\n",
    "normalization_layer = tf.keras.layers.Normalization()\n",
    "normalization_layer.adapt(train_values)\n",
    "# Create a simple NN\n",
    "model_layers = [\n",
    "    normalization_layer,\n",
    "    tf.keras.layers.Dense(60, activation='relu'),\n",
    "    tf.keras.layers.Dense(60, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "]\n",
    "model = tf.keras.models.Sequential(model_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cadbd3f",
   "metadata": {},
   "source": [
    "## Train the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bee8e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "# Optimizer\n",
    "adam_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0002, beta_1=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748a3cfd",
   "metadata": {},
   "source": [
    "To monitore the training with weights one should use the weighted metric. This metric is calculated with the provided weights of the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d02f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model now with the weighted metric\n",
    "model.compile(optimizer=adam_optimizer, loss=loss_fn, weighted_metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8335600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GPnLZ6V0bAjX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GPnLZ6V0bAjX",
    "outputId": "765d4daa-c23b-400b-fc7d-018733ef46e1"
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = model.fit(train_data, validation_data=val_data, callbacks=[early_stopping], epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Iyh3S_PZR91J",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "id": "Iyh3S_PZR91J",
    "outputId": "771ec678-30f1-4c79-edb1-b46f1afb7b23"
   },
   "outputs": [],
   "source": [
    "# Plot the training history\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "ax.plot(history.history['loss'], label='training')\n",
    "ax.plot(history.history['val_loss'], label='validation')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('loss')\n",
    "ax.legend()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f55f3b",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Explain the difference in the training loss when the weights are applied.\n",
    "</font>\n",
    "\n",
    "<font color='green'>\n",
    "Answer:\n",
    "\n",
    "For the calculation of the loss the weights are applied for each event. Since the mean training weight is $4.45 \\cdot 10^{-4}$ the loss is reduced by the same magnitude.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcef422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The mean training weight is {np.round(train_weights.mean(), 6)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb039355",
   "metadata": {},
   "source": [
    "## Apply and Evaluate the Neural Network on Training and Validation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f21dd67",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "- Apply the model on the training and validation data and plot the classification\n",
    "- Evaluate the loss and accuracy for the training and validation data\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VcSLHpwBbAtP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "id": "VcSLHpwBbAtP",
    "outputId": "1f900781-0616-4cff-8c6c-0bfb89f95a87"
   },
   "outputs": [],
   "source": [
    "# Apply the model for training and validation values\n",
    "train_prediction = model.predict(train_values)\n",
    "val_prediction = model.predict(val_values)\n",
    "\n",
    "# Plot the model output\n",
    "common.plot_dnn_output(train_prediction, train_classification, val_prediction, val_classification)\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dG_-a34VIKm0",
   "metadata": {
    "id": "dG_-a34VIKm0"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on training and validation data\n",
    "model_train_evaluation = model.evaluate(train_data)\n",
    "model_val_evaluation = model.evaluate(val_data)\n",
    "\n",
    "print(f'train loss = {round(model_train_evaluation[0], 5)}\\ttrain binary accuracy = {round(model_train_evaluation[1], 5)}')\n",
    "print(f'val loss = {round(model_val_evaluation[0], 5)}\\tval binary accuracy = {round(model_val_evaluation[1], 5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860c3532",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "What differences can you see on the accuarcy and classification distribution? Explain these differences.\n",
    "</font>\n",
    "\n",
    "<font color='green'>\n",
    "Answer:\n",
    "\n",
    "The accuracy has significantly improved to 97.5% during the whole training. The reason for this can be seen in the classification plots. Since the weights of signal events are much smaller the model focuses mostly on the classification of background events during the training and therefore classifies all events close to background. With a signal purity of about 2.5% this leads to a accuracy of 97.5%.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe1d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the event weights\n",
    "mean_signal_weight = weights[classification > 0.5].mean()\n",
    "mean_background_weight = weights[classification < 0.5].mean()\n",
    "print(f'The mean weight for signal events is {np.round(mean_signal_weight, 7)} and for background events {np.round(mean_background_weight, 7)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66701ee4",
   "metadata": {},
   "source": [
    "## Reweight the Event Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d212439d",
   "metadata": {},
   "source": [
    "Introducing the event weights caused the problem that the overall background dominates the training. With the event weights of the simulated data the loss is already quite low if all data is consequently classified close to background.<br>\n",
    "$H = -\\frac{1}{N} \\sum_i^N w_i (y_i^{true} log(y_i^{predict}) + (1 - y_i^{true}) log(1 - y_i^{predict}))$\n",
    "\n",
    "To solve this problem we should reweight the weights to have the same effect on training for signal and background events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3753c4ab",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Write a function that reweights the given weights:<br>\n",
    "1. Take the absolute value of the weight to not run into problems with negative weights\n",
    "2. Split the weights into signal and background weights (by setting the other weights in the array to zero)\n",
    "3. Scale the signal weights to have the total sum as the background weights\n",
    "4. Merge the background and scaled signal weights\n",
    "5. Scale all weights to have a mean weight of 1\n",
    "</font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MoHxpNYmQSiL",
   "metadata": {
    "id": "MoHxpNYmQSiL"
   },
   "outputs": [],
   "source": [
    "def reweight_weights(weights, classification):\n",
    "    # Take the absolute value of the weight\n",
    "    weights_abs = np.abs(weights)\n",
    "    # Split in signal and background weights\n",
    "    weights_signal = weights_abs*classification\n",
    "    weights_background = weights_abs*(1 - classification)\n",
    "    # Scale the signal events\n",
    "    weights_signal_scaled = weights_signal * sum(weights_background) / sum(weights_signal)\n",
    "    # Merge the signal and background events\n",
    "    weights_reweighted = weights_background + weights_signal_scaled\n",
    "    # Normalize mean weight to one\n",
    "    weights_reweighted /= weights_reweighted.mean()\n",
    "    return weights_reweighted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2035fe",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Test your reweighting function. The mean of all weights should be 1 and the sum of signal weights and background weights should be equal.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f192df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weights_reweighted = reweight_weights(weights, classification)\n",
    "signal_weights_reweighted = weights_reweighted[classification > 0.5]\n",
    "background_weights_reweighted = weights_reweighted[classification < 0.5]\n",
    "\n",
    "print(f'Mean weight: {weights_reweighted.mean()}')\n",
    "print(f'Signal weight sum: {signal_weights_reweighted.sum()}')\n",
    "print(f'Background weight sum: {background_weights_reweighted.sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fdf77f",
   "metadata": {},
   "source": [
    "When you are happy with your reweighting function save it for the use in the next chapters.<br>\n",
    "If you have used any imported modules first save these imports to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618df050",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile functions/reweight_weights.py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766a9260",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import getsource, getmodulename\n",
    "%save -a functions/reweight_weights.py getsource(reweight_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33460630",
   "metadata": {},
   "source": [
    "## Recreate and Retrain the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142a85c1",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Create tensorflow datasets with the reweighted event weights for training and validation.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5mTwdrlOQSzZ",
   "metadata": {
    "id": "5mTwdrlOQSzZ"
   },
   "outputs": [],
   "source": [
    "# Get reweighted weights\n",
    "train_weights_reweighted = reweight_weights(train_weights, train_classification)\n",
    "val_weights_reweighted = reweight_weights(val_weights, val_classification)\n",
    "\n",
    "# Convert the data to tensorflow datasets\n",
    "train_data = Dataset.from_tensor_slices((train_values, train_classification, train_weights_reweighted))\n",
    "train_data = train_data.shuffle(len(train_data), seed=random_state)\n",
    "train_data = train_data.batch(128)\n",
    "val_data = Dataset.from_tensor_slices((val_values, val_classification, val_weights_reweighted))\n",
    "val_data = val_data.shuffle(len(val_data), seed=random_state)\n",
    "val_data = val_data.batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dBrso_7_QTDe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dBrso_7_QTDe",
    "outputId": "1c70bb48-1113-41da-8c86-351df614ea94"
   },
   "outputs": [],
   "source": [
    "# Normalization layer\n",
    "normalization_layer = tf.keras.layers.Normalization()\n",
    "normalization_layer.adapt(train_values)\n",
    "# Create a simple NN\n",
    "model_layers = [\n",
    "    normalization_layer,\n",
    "    tf.keras.layers.Dense(60, activation='relu'),\n",
    "    tf.keras.layers.Dense(60, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "]\n",
    "model = tf.keras.models.Sequential(model_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bab2f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(optimizer=adam_optimizer, loss=loss_fn, weighted_metrics=['binary_accuracy'])\n",
    "# Train model\n",
    "history = model.fit(train_data, validation_data=val_data, callbacks=[early_stopping], epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pOnxDoVXQTT4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "pOnxDoVXQTT4",
    "outputId": "12eba90b-923e-4328-e304-00253e772ae1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the training history\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "ax.plot(history.history['loss'], label='training')\n",
    "ax.plot(history.history['val_loss'], label='validation')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('loss')\n",
    "ax.legend()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305c285b",
   "metadata": {},
   "source": [
    "Since you reweighted the mean training and validation weight to one the loss is in the same magnetude as in the chapters before. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a9396b",
   "metadata": {},
   "source": [
    "## Apply and Evaluate the Neural Network on Training and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ap3ul7SQTzJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "id": "7ap3ul7SQTzJ",
    "outputId": "fc47ce20-fe1f-4a32-80aa-1e525ce686a7"
   },
   "outputs": [],
   "source": [
    "# Apply the model for training and validation values\n",
    "train_prediction = model.predict(train_values)\n",
    "val_prediction = model.predict(val_values)\n",
    "# Plot the model output\n",
    "common.plot_dnn_output(train_prediction, train_classification, val_prediction, val_classification)\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adc25b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on training and validation data\n",
    "model_train_evaluation = model.evaluate(train_data)\n",
    "model_val_evaluation = model.evaluate(val_data)\n",
    "\n",
    "print(f'train loss = {round(model_train_evaluation[0], 5)}\\ttrain binary accuracy = {round(model_train_evaluation[1], 5)}')\n",
    "print(f'val loss = {round(model_val_evaluation[0], 5)}\\tval binary accuracy = {round(model_val_evaluation[1], 5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b88e81",
   "metadata": {},
   "source": [
    "## Prediction on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mPQ9yQ7TSMnh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 777
    },
    "id": "mPQ9yQ7TSMnh",
    "outputId": "c9611a6b-ccea-4a06-b32f-540d34a170e8"
   },
   "outputs": [],
   "source": [
    "# Apply the model on the test data\n",
    "data_frames_apply_dnn = common.apply_dnn_model(model, test_data_frames, training_variables, sample_list_signal + sample_list_background)\n",
    "model_prediction = {'variable': 'model_prediction',\n",
    "                    'binning': np.linspace(0, 1, 50),\n",
    "                    'xlabel': 'prediction'}\n",
    "common.plot_hist(model_prediction, data_frames_apply_dnn, show_data=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefa7d17",
   "metadata": {
    "id": "nSH3fPCBSM6j"
   },
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Compare this classification plot with the one resulting from the training without event weights in chapter 2.\n",
    "</font>\n",
    "\n",
    "<font color='green'>\n",
    "Answer:\n",
    "\n",
    "The overall classification of signal and $llll$ background events looks quite similar to the distribution in chapter 2. However, the classification of backgrounds modelled by a low number of generated events has significantly improved.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5baac48",
   "metadata": {},
   "source": [
    "## Save and Load a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486f1b0c",
   "metadata": {},
   "source": [
    "Save the model for a later comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eecf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/chapter5_model')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
