{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2081cce3",
   "metadata": {
    "id": "2081cce3"
   },
   "source": [
    "# Discover the Higgs with Deep Neural Networks\n",
    "# Chapter 6: Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78f07e1",
   "metadata": {},
   "source": [
    "In this chapter the concept of cross-validation is introduced to evaluate the network performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244f5820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from numpy.random import seed\n",
    "import os\n",
    "\n",
    "# Import the tensorflow module to create a neural network\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "# Import function to split data into train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import some common functions created for this notebook\n",
    "import common\n",
    "\n",
    "# Random state\n",
    "random_state = 21\n",
    "_ = np.random.RandomState(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600a4cea",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b61d26",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15227a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input samples\n",
    "sample_list_signal = ['ggH125_ZZ4lep', 'VBFH125_ZZ4lep', 'WH125_ZZ4lep', 'ZH125_ZZ4lep']\n",
    "sample_list_background = ['llll', 'Zee', 'Zmumu', 'ttbar_lep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f49139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = 'input'\n",
    "# Read all the samples\n",
    "no_selection_data_frames = {}\n",
    "for sample in sample_list_signal + sample_list_background:\n",
    "    no_selection_data_frames[sample] = pd.read_csv(os.path.join(sample_path, sample + '.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd182701",
   "metadata": {},
   "source": [
    "### Event Pre-Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a45fe8a",
   "metadata": {},
   "source": [
    "Import the pre-selection functions saved during the first chapter. If the modules are not found solve and execute the notebook of the first chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be0e5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.selection_lepton_charge import selection_lepton_charge\n",
    "from functions.selection_lepton_type import selection_lepton_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae94f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original data frame to investigate later\n",
    "data_frames = no_selection_data_frames.copy()\n",
    "\n",
    "# Apply the chosen selection criteria\n",
    "for sample in sample_list_signal + sample_list_background:\n",
    "    # Selection on lepton type\n",
    "    type_selection = np.vectorize(selection_lepton_type)(\n",
    "        data_frames[sample].lep1_pdgId,\n",
    "        data_frames[sample].lep2_pdgId,\n",
    "        data_frames[sample].lep3_pdgId,\n",
    "        data_frames[sample].lep4_pdgId)\n",
    "    data_frames[sample] = data_frames[sample][type_selection]\n",
    "\n",
    "    # Selection on lepton charge\n",
    "    charge_selection = np.vectorize(selection_lepton_charge)(\n",
    "        data_frames[sample].lep1_charge,\n",
    "        data_frames[sample].lep2_charge,\n",
    "        data_frames[sample].lep3_charge,\n",
    "        data_frames[sample].lep4_charge)\n",
    "    data_frames[sample] = data_frames[sample][charge_selection]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d11b53e",
   "metadata": {},
   "source": [
    "### Get Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608219a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to keep 40% for testing\n",
    "train_data_frames, test_data_frames = common.split_data_frames(data_frames, 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48f6205",
   "metadata": {},
   "source": [
    "Import the reweighting function to train with event weights. If the module is not found solve and execute the notebook of chapter 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0303d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.reweight_weights import reweight_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DnaOe4OGlaL9",
   "metadata": {
    "id": "DnaOe4OGlaL9"
   },
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aa7519",
   "metadata": {},
   "source": [
    "If you have a look on the training history you see fluctuations in the validation loss. Furthermore, the validation dataset also has a limited size, making it potentially not completely representative for validation.\n",
    "\n",
    "So how should one evaluate the performance of a model and compare two models?\n",
    "\n",
    "A commonly used method to evaluate the model performance is k-fold.\n",
    "The training data is split several times with non-overlapping validation sets. On each split a model is trained and validated on the corresponding validation data. This results into several independently trained models with same size and setup validated on different datasets. Thus, one is able to calculate the mean performance of the resulting models.\n",
    "\n",
    "<div>\n",
    "<img src='figures/kFold.png' width='400'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbee2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training input variables\n",
    "training_variables = ['lep1_pt', 'lep2_pt', 'lep3_pt', 'lep4_pt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e98c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the values, weights, and classification of the data\n",
    "values, weights, classification = common.get_dnn_input(train_data_frames, training_variables, sample_list_signal, sample_list_background)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6624d93",
   "metadata": {},
   "source": [
    "Use kFold to split the data 3 times in 2/3 training and 1/3 validation data each randomly shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p2YYk5F9SNJl",
   "metadata": {
    "id": "p2YYk5F9SNJl"
   },
   "outputs": [],
   "source": [
    "# Import the kFold module for cross-validation\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dI3Zp2xXSPri",
   "metadata": {
    "id": "dI3Zp2xXSPri"
   },
   "outputs": [],
   "source": [
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d33e0c",
   "metadata": {},
   "source": [
    "Now use kFold to train several models in a for loop. In each iteration you have to create a new model and train it. This results into three models trained and validated on overlapping training sets and not overlapping validation sets.\n",
    "\n",
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Fill the missing parts in the for loop:\n",
    "- Reweight the weights\n",
    "- Convert the values and classification into tensorflow datasets\n",
    "- Create a model with normalization layer and 2 hidden layers with 60 nodes each\n",
    "- Compile the model\n",
    "- Train the model with early stopping\n",
    "- Evaluate the model on the training and evaluation data\n",
    "</font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8c8681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "# Optimizer\n",
    "adam_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0002, beta_1=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13de72a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "--Jm5SPaSQFm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "--Jm5SPaSQFm",
    "outputId": "73cef139-4304-4936-ff12-025a66482277"
   },
   "outputs": [],
   "source": [
    "# Store the models and their training history\n",
    "kfold_history = []\n",
    "kfold_model = []\n",
    "# Store the evaluation on training and validation data\n",
    "kfold_train_eval_loss = []\n",
    "kfold_train_eval_acc = []\n",
    "kfold_val_eval_loss = []\n",
    "kfold_val_eval_acc = []\n",
    "split_idx = 1\n",
    "for train_indices, val_indices in kfold.split(values):\n",
    "    print(f'Use fold {split_idx}')\n",
    "    split_idx += 1\n",
    "    # Get train and validation data \n",
    "    train_values = values[train_indices]\n",
    "    train_classification = classification[train_indices]\n",
    "    train_weights = weights[train_indices]\n",
    "    val_values = values[val_indices]\n",
    "    val_classification = classification[val_indices]\n",
    "    val_weights = weights[val_indices]\n",
    "    # Get reweighted weights\n",
    "    train_weights_reweighted = reweight_weights(train_weights, train_classification)\n",
    "    val_weights_reweighted = reweight_weights(val_weights, val_classification)\n",
    "    # Get train and validation datasets\n",
    "    train_data = Dataset.from_tensor_slices((train_values, train_classification, train_weights_reweighted))\n",
    "    train_data = train_data.shuffle(len(train_data), seed=random_state)\n",
    "    train_data = train_data.batch(124)\n",
    "    val_data = Dataset.from_tensor_slices((val_values, val_classification, val_weights_reweighted))\n",
    "    val_data = val_data.shuffle(len(val_data), seed=random_state)\n",
    "    val_data = val_data.batch(124)\n",
    "\n",
    "    # Normalization layer\n",
    "    normalization_layer = tf.keras.layers.Normalization()\n",
    "    normalization_layer.adapt(train_values)\n",
    "    # Create a simple NN\n",
    "    model_layers = [\n",
    "        normalization_layer,\n",
    "        tf.keras.layers.Dense(60, activation='relu'),\n",
    "        tf.keras.layers.Dense(60, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "    ]\n",
    "    model = tf.keras.models.Sequential(model_layers)\n",
    "    # Compile model\n",
    "    model.compile(optimizer=adam_optimizer, loss=loss_fn, weighted_metrics=['binary_accuracy'])\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(train_data, validation_data=val_data, callbacks=[early_stopping], epochs=1000)\n",
    "\n",
    "    # Append to list\n",
    "    kfold_history.append(history)\n",
    "    kfold_model.append(model)\n",
    "\n",
    "    # Evaluate model on training and validation data\n",
    "    model_train_evaluation = model.evaluate(train_data)\n",
    "    model_val_evaluation = model.evaluate(val_data)\n",
    "    kfold_train_eval_loss.append(model_train_evaluation[0])\n",
    "    kfold_train_eval_acc.append(model_train_evaluation[1])\n",
    "    kfold_val_eval_loss.append(model_val_evaluation[0])\n",
    "    kfold_val_eval_acc.append(model_val_evaluation[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a87fdf8",
   "metadata": {},
   "source": [
    "Lets plot the training history of the three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L1pKiwRvrcok",
   "metadata": {
    "id": "L1pKiwRvrcok",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the training history\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "color_list = ['r', 'g', 'b']\n",
    "for k_fold_idx, (history, color) in enumerate(zip(kfold_history, color_list)):\n",
    "  ax.plot(history.history['loss'], color=color, label=f'{k_fold_idx} training')\n",
    "  ax.plot(history.history['val_loss'], color=color, ls='--', label=f'{k_fold_idx} val')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('loss')\n",
    "ax.legend()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80add66e",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Calculate the mean and std of the validation loss.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad5ca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_mean = np.mean(kfold_val_eval_loss)\n",
    "val_loss_std = np.std(kfold_val_eval_loss)\n",
    "print(f'The val loss of the model is {round(val_loss_mean, 3)} +- {round(val_loss_std, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a38154",
   "metadata": {},
   "source": [
    "## Save and Load a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907eab0a",
   "metadata": {},
   "source": [
    "Lets save the three models of the cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5145262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all models\n",
    "for idx, model in enumerate(kfold_model):\n",
    "    # Save the model\n",
    "    model.save(f'models/chapter6_model_crossval{idx}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
