{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2081cce3",
   "metadata": {
    "id": "2081cce3"
   },
   "source": [
    "# Discover the Higgs with Deep Neural Networks\n",
    "# Chapter 7: Application for Higgs Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78f07e1",
   "metadata": {},
   "source": [
    "In this chapter we will use the neural network of the last chapters to search for the higgs boson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244f5820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from numpy.random import seed\n",
    "import os\n",
    "\n",
    "# Import the tensorflow module to create a neural network\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "# Import function to split data into train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import some common functions created for this notebook\n",
    "import common\n",
    "\n",
    "# Random state\n",
    "random_state = 21\n",
    "_ = np.random.RandomState(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600a4cea",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b61d26",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15227a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input samples\n",
    "sample_list_signal = ['ggH125_ZZ4lep', 'VBFH125_ZZ4lep', 'WH125_ZZ4lep', 'ZH125_ZZ4lep']\n",
    "sample_list_background = ['llll', 'Zee', 'Zmumu', 'ttbar_lep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f49139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = 'input'\n",
    "# Read all the samples\n",
    "no_selection_data_frames = {}\n",
    "for sample in sample_list_signal + sample_list_background:\n",
    "    no_selection_data_frames[sample] = pd.read_csv(os.path.join(sample_path, sample + '.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd182701",
   "metadata": {},
   "source": [
    "### Event Pre-Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a45fe8a",
   "metadata": {},
   "source": [
    "Import the pre-selection functions saved during the first chapter. If the modules are not found solve and execute the notebook of the first chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be0e5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.selection_lepton_charge import selection_lepton_charge\n",
    "from functions.selection_lepton_type import selection_lepton_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae94f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original data frame to investigate later\n",
    "data_frames = no_selection_data_frames.copy()\n",
    "\n",
    "# Apply the chosen selection criteria\n",
    "for sample in sample_list_signal + sample_list_background:\n",
    "    # Selection on lepton type\n",
    "    type_selection = np.vectorize(selection_lepton_type)(\n",
    "        data_frames[sample].lep1_pdgId,\n",
    "        data_frames[sample].lep2_pdgId,\n",
    "        data_frames[sample].lep3_pdgId,\n",
    "        data_frames[sample].lep4_pdgId)\n",
    "    data_frames[sample] = data_frames[sample][type_selection]\n",
    "\n",
    "    # Selection on lepton charge\n",
    "    charge_selection = np.vectorize(selection_lepton_charge)(\n",
    "        data_frames[sample].lep1_charge,\n",
    "        data_frames[sample].lep2_charge,\n",
    "        data_frames[sample].lep3_charge,\n",
    "        data_frames[sample].lep4_charge)\n",
    "    data_frames[sample] = data_frames[sample][charge_selection]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cd8f99",
   "metadata": {},
   "source": [
    "### Get Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d34354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to keep 40% for testing\n",
    "train_data_frames, test_data_frames = common.split_data_frames(data_frames, 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DnaOe4OGlaL9",
   "metadata": {
    "id": "DnaOe4OGlaL9"
   },
   "source": [
    "## Statistical Significance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9c2ccf",
   "metadata": {},
   "source": [
    "In the search for new physics, the question arises whether one has really discovered something new or it was just a random fluctuation in the data. If, for example, 25 events are expected, but 30 events are measured, is this simply coincidence or are there unknown phenomena behind it? This decision can be made by using the significance.\n",
    "\n",
    "First, a null hypothesis must be chosen. This hypothesis can then be either rejected or held by the measurement. However, a final confirmation of a hypothesis is not possible. A final confirmation of a hypothesis is however not possible, since one can never be finally sure that deviations from the hypothesis cannot exist. or our measurement we choose the following null hypothesis:\n",
    "\n",
    "$H_0$: The Higgs boson does not exist and the measurement is fully described by teh backgrounds.\n",
    "\n",
    "Now we assume for the moment that the null hypothesis $H_0$ is correct. Under this assumption, we calculate the probability for results that deviate at least as much from the null hypothesis $H_0$ as the actual measurement. Applied to the above example, this means that one would expect 25 events and calculates the probability of a deviation of more than 5 events. The probability distribution for such counting experiments is discribed by the Poisson distribution. Its expectation value $\\mu$ is given by the prediction $N_{pred}$ and the standard deviation $\\sigma$ is given by $\\sqrt{N_{pred}}$. If the number of expected events is high enough the Poisson distribution more and more gaussian.\n",
    "\n",
    "The visualization of this probability function for $N_{pred} = 25$ can be seen in the following plot. The probability (p-value) of a deviation of more than 5 events is 32%. This means that if our null hypothesis of 25 events is correct, the probability of measurements more extrem than 30 events is 32%. Thus it is quite likely that this deviation is only a fluctuation and the null hypothesis of 25 events can be held.\n",
    "\n",
    "<div>\n",
    "<img src='figures/significance_pred_25_meas_30.png' width='500'/>\n",
    "</div>\n",
    "\n",
    "Lets assume that we have measured 35 events. The probability of such a fluctuation would be about 4.6%. In many scientific studies like in medicine, null hypotheses are rejected with a p-value below 5%. \n",
    "<div>\n",
    "<img src='figures/significance_pred_25_meas_35.png' width='500'/>\n",
    "</div>\n",
    "\n",
    "If one would measure now 40 events, this would correspond to a p-value of 0.3%. The null hypothesis could still be true, but the probability that this measurement was only a fluctuation is very low.\n",
    "\n",
    "<div>\n",
    "<img src='figures/significance_pred_25_meas_40.png' width='500'/>\n",
    "</div>\n",
    "\n",
    "Instead of the p-value, the deviations from the p-value are also often given in standard deviations. The resulting significance $Z$ is given by the number of standard deviations by which the measured value deviates from the prediction.<br>\n",
    "Thus, the statistical significance $Z$ is given by:<br>\n",
    "$Z_{stat} = \\frac{|N_{pred} - N_{meas}|}{\\sqrt{N_{pred}}}$\n",
    "\n",
    "For our previous examples, the following significances result:\n",
    "- $N_{pred} = 25$ and $N_{meas} = 30$ $\\rightarrow$ $Z_{stat} = 1$\n",
    "- $N_{pred} = 25$ and $N_{meas} = 35$ $\\rightarrow$ $Z_{stat} = 2$\n",
    "- $N_{pred} = 25$ and $N_{meas} = 40$ $\\rightarrow$ $Z_{stat} = 3$\n",
    "\n",
    "<b>This calculation of the significance is only an approximation. If the number of predicted events becomes too low the approximation failes. Therefore, one should not use this approximation for $N_{pred}$ of less than 10 events.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ddbc7f",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "In the first chapter we have observed a prediction of 390.6 background events and 9.7 Higgs events. The prediction without the Higgs boson is our null hypothesis $H_0$. Which significance can we expect for a measurement with Higgs events? Would you reject the null hypothesis?\n",
    "</font>\n",
    "\n",
    "<font color='green'>\n",
    "Answer:\n",
    "\n",
    "For the significance calculation we have $N_{pred} = N_{bkg}$ and $N_{meas} = N_{bkg} + N_{Higgs}$:<br>\n",
    "$Z_{stat} = \\frac{|N_{bkg} - (N_{bkg} + N_{Higgs})|}{\\sqrt{N_{bkg}}}$<br>\n",
    "$Z_{stat} = \\frac{N_{Higgs}}{\\sqrt{N_{bkg}}}$<br>\n",
    "$Z_{stat} = 0.49$\n",
    "    \n",
    "A measurement with Higgs boson would deviate only by half a standard deviation from the background-only prediction. Thus, it is quite likely that this deviation is only a fluctuation and we cannot claim a Higgs observation.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078a424f",
   "metadata": {},
   "source": [
    "## Higgs Measurement with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fb848d",
   "metadata": {},
   "source": [
    "Since the significance on the sum of all events is very low, we now apply our neuron networks to boost this sensitivity. To improve the significance $Z$ there are two options, increase the Higgs signal or decrese the backgrounds. Since our data is fix we won't get more Higgs events and have to decrease the background contribution. This can be realized by the classification resulting from our neuron networks. For each event the neural network returns a score between 0 and 1 and the closer the score is to 1 the higher is the probability that it is a Higgs event. Thus, we can apply a cut value similar to the preselection in chapter 1. We will only use the events with a classification score higher than the cut value for the significance calculation.\n",
    "\n",
    "<div>\n",
    "<img src='figures/significance_cut_value.png' width='500'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a48f6cf",
   "metadata": {},
   "source": [
    "To avoid a bias resulting from the training we will only use the test data frame not used for any training so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffac8f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training input variables\n",
    "training_variables = ['lep1_pt', 'lep2_pt', 'lep3_pt', 'lep4_pt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217bbdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the values, weights, and classification of the test dataset\n",
    "test_values, test_weights, test_classification = common.get_dnn_input(test_data_frames, training_variables, sample_list_signal, sample_list_background)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf57b6a7",
   "metadata": {},
   "source": [
    "For the significance calculation we split the test data into signal and bakcground events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7c6b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data in signal and background\n",
    "test_signal_values = test_values[test_classification > 0.5]\n",
    "test_signal_weights = test_weights[test_classification > 0.5]\n",
    "test_bkg_values = test_values[test_classification < 0.5]\n",
    "test_bkg_weights = test_weights[test_classification < 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b4a6b0",
   "metadata": {},
   "source": [
    "Now we can use our neural networks to improve the significance. Lets try this procedure for our very first model created in chapter 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6e4162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models of chapter 2\n",
    "model_chapter2 = tf.keras.models.load_model('models/chapter2_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bead4b",
   "metadata": {},
   "source": [
    "As in the chapters before we apply our model but now seperately for signal and background events. In order to simplify the next steps we transform this prediction into one dimensional numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21d43f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model prediction from chapter 2\n",
    "test_signal_chapter2_prediction = model_chapter2.predict(test_signal_values)\n",
    "test_bkg_chapter2_prediction = model_chapter2.predict(test_bkg_values)\n",
    "\n",
    "# Transform predicton to array\n",
    "test_signal_chapter2_prediction = np.array([element[0] for element in test_signal_chapter2_prediction])\n",
    "test_bkg_chapter2_prediction = np.array([element[0] for element in test_bkg_chapter2_prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9f6fab",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "In the following cell you can find the significance calculation for a given cut value. Vary the cut value and describe which effects you can see.\n",
    "</font>\n",
    "\n",
    "<font color='green'>\n",
    "Answer:\n",
    "\n",
    "A cut value of 0 is passed by all events and thus we get the significance we have calculated for the full data set.<br>\n",
    "A cut value of 0.5 rejects already 69% of the background events while keeping 81% of the Higgs events resulting in a significance of 0.7 sigma.<br>\n",
    "However, the higher the cut value is chosen the less Higgs events can pass it. Thus the significance improvement reaches its limit at a certain point. At a cut value of 0.8 only 2.9 Higgs events are expected to enter the significance calculation.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7176ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_value = 0.0\n",
    "# Number of signal and background events passing the prediction selection\n",
    "n_signal = test_signal_weights[test_signal_chapter2_prediction > cut_value].sum()\n",
    "n_bkg = test_bkg_weights[test_bkg_chapter2_prediction > cut_value].sum()\n",
    "\n",
    "# Significance\n",
    "significance = n_signal / np.sqrt(n_bkg)\n",
    "\n",
    "print(f'The prediction selection is passed by {round(n_signal, 2)} signal and {round(n_bkg, 2)} background events.')\n",
    "print(f'This results in a significance of {round(significance, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78682a9c",
   "metadata": {},
   "source": [
    "So what would be the best cut value and its corresponding significance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cffa47",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Define a function which applies a model on given signal and background events and calculates the significance for different cut values. Do the calculation in a for loop and break if the number of background events is lower than 10. Apply this function for the model of chapter 2.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d123998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_significances(model, signal_values, bkg_values, signal_weights, bkg_weights):\n",
    "    # Model prediction\n",
    "    signal_prediction = model.predict(signal_values)\n",
    "    bkg_prediction = model.predict(bkg_values)\n",
    "\n",
    "    # Transform predicton to array\n",
    "    signal_prediction = np.array([element[0] for element in signal_prediction])\n",
    "    bkg_prediction = np.array([element[0] for element in bkg_prediction])\n",
    "    \n",
    "    # Calculate the significance for different cut values in a for loop\n",
    "    cut_values = []\n",
    "    significances = []\n",
    "    for cut_value in np.linspace(0, 1, 1000):\n",
    "        # Number of signal and background events passing the prediction selection\n",
    "        n_signal = signal_weights[signal_prediction > cut_value].sum()\n",
    "        n_bkg = bkg_weights[bkg_prediction > cut_value].sum()\n",
    "\n",
    "        # Break if less than 10 background events\n",
    "        if n_bkg < 10:\n",
    "            break\n",
    "\n",
    "        # Significance calculation\n",
    "        significance = n_signal / np.sqrt(n_bkg)\n",
    "        \n",
    "        # Append the cut value and the significances to their lists\n",
    "        cut_values.append(cut_value)\n",
    "        significances.append(significance)\n",
    "    return cut_values, significances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7786bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the significances by the model of chapter 2\n",
    "model_chapter2_cut_values, model_chapter2_significances = get_significances(model_chapter2, test_signal_values, test_bkg_values, test_signal_weights, test_bkg_weights)\n",
    "print(model_chapter2_significances[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1901e8ec",
   "metadata": {},
   "source": [
    "Save this function for the next chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665783d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile functions/get_significances.py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f287e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import getsource, getmodulename\n",
    "%save -a functions/get_significances.py getsource(get_significances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697adb23",
   "metadata": {},
   "source": [
    "Now lets plot the significance for different cut values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b08535",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the significances\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "ax.plot(model_chapter2_cut_values, model_chapter2_significances)\n",
    "ax.set_title('Significances for model of chapter 2')\n",
    "ax.set_xlabel('cut at prediction value')\n",
    "ax.set_ylabel('significance')\n",
    "ax.set_xlim(0, 1)\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72db6706",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "What is the best significance one get by the model of chapter 2?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad46af3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'The best significance by the model of chapter 2 is {round(max(model_chapter2_cut_values), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdfac6b",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Lets assume you have an extrem powerfull nerual network for the Higgs search. What would be the best possible significance you could get?\n",
    "</font>\n",
    "\n",
    "Hint: You still need at least 10 backgfround events to apply our significance calculation.\n",
    "\n",
    "<font color='green'>\n",
    "Answer:\n",
    "\n",
    "The best possible significance with 10 background events would be:<br>\n",
    "$Z_{stat;best} = \\frac{9.7}{\\sqrt{10}} = 3.07$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63db2652",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Load the neural networks created in chapter 4 and chapter 5 and calculate their significances for different cut values. Compare the significances for all of the three models in one plot. Describe what you can see and compare their maximal significances.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b87287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models of chapter 4 and chapter 5\n",
    "model_chapter4 = tf.keras.models.load_model('models/chapter4_model')\n",
    "model_chapter5 = tf.keras.models.load_model('models/chapter5_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ea6fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the significances by the model of chapter 4 and chapter 5\n",
    "model_chapter4_cut_values, model_chapter4_significances = get_significances(model_chapter4, test_signal_values, test_bkg_values, test_signal_weights, test_bkg_weights)\n",
    "model_chapter5_cut_values, model_chapter5_significances = get_significances(model_chapter5, test_signal_values, test_bkg_values, test_signal_weights, test_bkg_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d20df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the significances\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "ax.plot(model_chapter2_cut_values, model_chapter2_significances, label='chapter 2: first model')\n",
    "ax.plot(model_chapter4_cut_values, model_chapter4_significances, label='chapter 4: early stopping')\n",
    "ax.plot(model_chapter5_cut_values, model_chapter5_significances, label='chapter 5: event weights')\n",
    "ax.set_xlabel('cut value')\n",
    "ax.set_ylabel('significance')\n",
    "ax.set_xlim(0, 1)\n",
    "ax.legend()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db75c2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The best significance by the model of chapter 2 is {round(max(model_chapter2_cut_values), 3)}')\n",
    "print(f'The best significance by the model of chapter 4 is {round(max(model_chapter4_cut_values), 3)}')\n",
    "print(f'The best significance by the model of chapter 5 is {round(max(model_chapter5_cut_values), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6c3f99",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "Answer:\n",
    "\n",
    "All three significance distributions start at 0.49 and reach their maximum around a cut value of 0.75. The distributions for the models of chapter 2 and chapter 4 appear very similar at first sight. If one considers that in chapter 4 1/3 of the data was no longer used for training but for performance validation, a significance decrease could be expected. However, this is compensated by the utilization of the optimal training duration. The clearest difference can be seen for the use of event weights. The neural network of chapter 5 results in the highest significance for all cut values and also its optimal significance is significantly higher than for the other two models.\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
