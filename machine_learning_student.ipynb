{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2081cce3",
   "metadata": {
    "id": "2081cce3"
   },
   "source": [
    "# Discover the Higgs with Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060ec659",
   "metadata": {
    "id": "060ec659"
   },
   "source": [
    "The input data was created from 13 TeV ATLAS open data available at http://opendata.atlas.cern/release/2020/documentation/index.html\n",
    "\n",
    "For more information read:<br>\n",
    "Review of the 13 TeV ATLAS Open Data release, Techn. Ber., All figures including auxiliary figures are available at https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PUBNOTES/ATL-OREACH-PUB-2020-001: CERN, 2020, url: http://cds.cern.ch/record/2707171\n",
    "\n",
    "The data is measured by the ATLAS detector, one of the four big detectors at the Large Hadron Collider (LHC) at the CERN research center:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06d649e",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src='figures/ATLAS_detector.png' width='700'/>\n",
    "</div>\n",
    "ATLAS Experiment Â© 2008 CERN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e383f77",
   "metadata": {},
   "source": [
    "The data analyzed in this jupyter notebook measured at a centre-of-mass energy of $\\sqrt{s}=13 \\text{ TeV}$ with an integrated luminosity of $10 \\text{ fb}^{-1}$ in the year 2016. To search for H$\\rightarrow$ZZ$\\rightarrow$llll events only events with four reconstructed leptons in the final state are included in the given datasets. The events will have a event signature similar to:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23df3294",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src='figures/ATLAS_four_lepton_event.png' width='700'/>\n",
    "</div>\n",
    "https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/CONFNOTES/ATLAS-CONF-2011-162/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771b117d",
   "metadata": {},
   "source": [
    "## Simulation and Event Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b321e326",
   "metadata": {},
   "source": [
    "In quantum physics, no concrete process outcomes are predicted, only their probabilities. In order to make a prediction for the measurement at the ATLAS detector, the frequencies must be simulated. For this purpose, random events are generated on the basis of the probability densities and then their respective measurement in the detector is simulated. Generating the expected number of events results in the following distribution for the lepton with the largest transverse momentum. The distribution itself is again split into the different Higgs processes (ggH125_ZZ4lep, VBFH125_ZZ4lep, WH125_ZZ4lep and ZH125_ZZ4lep) and the background processes (llll, Zee, Zmumu and ttbar_lep).\n",
    "<div>\n",
    "<img src='figures/event_weights_few_not_applied.png' width='500'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c534be",
   "metadata": {},
   "source": [
    "This graphic already offers a good insight into how the data to be measured could be distributed. Unfortunately, the distribution is not very smooth due to the low number of events. To improve the prediction, more events are simulated than would actually be expected in the data. Especially for processes of high interest, like here the Higgs processes, especially many events are generated. The higher statistic results into much smoother predictions.\n",
    "<div>\n",
    "<img src='figures/event_weights_all_not_applied.png' width='500'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ec40e5",
   "metadata": {},
   "source": [
    "However, now both the ratios between the different processes have shifted and the prediction of the total events has increased extremely. This distribution of the \"raw\" simulation events thus no longer corresponds to what can be expected for the actual measurement. To correct this, event weights are applied. Thus, each simulated event will enter the distribution only as fraction of an event given by the respective event weight. This weight depends on the respective simulated process as well as on the kinematic region of the event. In addition, there are also negative weights to compensate for excess simulated events. The result is comparable to the initial distribution but offers a much smoother prediction.\n",
    "<div>\n",
    "<img src='figures/event_weights_all_applied.png' width='500'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e312205",
   "metadata": {
    "id": "3e312205"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5b2349",
   "metadata": {
    "id": "8f5b2349"
   },
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9615674",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f9615674",
    "outputId": "83e88f84-c0ec-4e77-e620-6bf415c816d1"
   },
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from numpy.random import seed\n",
    "import os\n",
    "\n",
    "# Random state\n",
    "random_state = 10\n",
    "_ = np.random.RandomState(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VSJXVaD1pS54",
   "metadata": {
    "id": "VSJXVaD1pS54"
   },
   "outputs": [],
   "source": [
    "import common"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3357ffaf",
   "metadata": {
    "id": "3357ffaf"
   },
   "source": [
    "The goal of this lab course is to train a deep neural network to separate Higgs boson signal from background events. The most important signal sample ggH125_ZZ4lep corresponds to the process gg$\\rightarrow$H$\\rightarrow$ZZ. The dominant background sample is llll resulting from Z and ZZ decays.\n",
    "After training the DNN model will be used to classify the events of the data samples.\n",
    "\n",
    "Higgs signal samples:\n",
    "- ggH125_ZZ4lep\n",
    "- VBFH125_ZZ4lep\n",
    "- WH125_ZZ4lep\n",
    "- ZH125_ZZ4lep\n",
    "\n",
    "Background samples:\n",
    "- llll\n",
    "- Zee\n",
    "- Zmumu\n",
    "- ttbar_lep\n",
    "\n",
    "Data samples:\n",
    "- data_A\n",
    "- data_B\n",
    "- data_C\n",
    "- data_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a2ae28",
   "metadata": {
    "id": "22a2ae28"
   },
   "outputs": [],
   "source": [
    "# Define the input samples\n",
    "sample_list_signal = ['ggH125_ZZ4lep']\n",
    "sample_list_background = ['llll']\n",
    "sample_list_signal = ['ggH125_ZZ4lep', 'VBFH125_ZZ4lep', 'WH125_ZZ4lep', 'ZH125_ZZ4lep']\n",
    "sample_list_background = ['llll', 'Zee', 'Zmumu', 'ttbar_lep']\n",
    "# sample_list_measured = ['data_A', 'data_B', 'data_C', 'data_D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b2ae67",
   "metadata": {
    "id": "e1b2ae67"
   },
   "outputs": [],
   "source": [
    "sample_path = 'input'\n",
    "# Read all the samples\n",
    "no_selection_data_frames = {}\n",
    "for sample in sample_list_signal + sample_list_background:\n",
    "    no_selection_data_frames[sample] = pd.read_csv(os.path.join(sample_path, sample + '.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a1549c",
   "metadata": {
    "id": "27a1549c"
   },
   "source": [
    "### Input Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe06218b",
   "metadata": {
    "id": "fe06218b"
   },
   "source": [
    "The input provides several variables to classify the events. Since each event has multiple leptons, they were ordered in descending order based on their transverse momentum. Thus, lepton 1 has the highest transverse momentum, lepton 2 the second highest, and so on. <br>\n",
    "Most of the given variables can be called low-level, because they represent event or object properties, which can be derived directly from the reconstruction in the detector. In contrast to this are high-level variables, which result from the combination of several low-level variables. In the given dataset the only high-level variables are invariant masses of multiple particles:<br>\n",
    "$m_{inv} = \\sqrt{\\left(\\sum\\limits_{i=1}^{n} E_i\\right)^2 - \\left(\\sum\\limits_{i=1}^{n} \\vec{p}_i\\right)^2}$\n",
    "\n",
    "\n",
    "List of all available variables:<br>\n",
    "- Scale and event weight\n",
    "     - The scaling for a dataset is given by the sum of event weights, the cross section, luminosity and a efficiency scale factor\n",
    "     - Each event has an additional specific event weight\n",
    "     - To combine simulated events and finally compare them to data each event has to be scaled by the event weight\n",
    "     - The weight are not used for training\n",
    "     - Variable name: `totalWeight`\n",
    "- Number of jets\n",
    "     - Jets are particle showers which result primarily from quarks and gluons\n",
    "     - Variable name: `jet_n`\n",
    "- Invariant four lepton mass\n",
    "     - The invariant mass $m_{inv}(l_1, l_2, l_3, l_4)$ is the reconstructed invariant mass of the full four lepton event.<br>\n",
    "     This variable is to be displayed later but not used for training.\n",
    "     - Variable name: `lep_m_llll`\n",
    "- Invariant two lepton mass\n",
    "     - Invariant masses $m_{inv}(l_i, l_j)$ of all combinations of two leptons\n",
    "     - Variable names: `lep_m_ll_12`, `lep_m_ll_13`, `lep_m_ll_14`, `lep_m_ll_23`, `lep_m_ll_24`, `lep_m_ll_34`\n",
    "- Transverse momentum $p_T$ of the leptons\n",
    "     - The momentum in the plane transverse to the beam axis\n",
    "     - Variable names: `lep1_pt`, `lep2_pt`, `lep3_pt`, `lep4_pt`\n",
    "- Lepton azimuthal angle\n",
    "     - The azimuthal angle $\\phi$ is measured in the plane transverse to the beam axis\n",
    "     - Variable name: `lep1_phi`, `lep2_phi`, `lep3_phi`, `lep4_phi`\n",
    "- Lepton pseudo rapidity\n",
    "     - The angle $\\theta$ is measured between the lepton track and the beam axis.<br>\n",
    "     Since this angle is not invariant against boosts along the beam axis, the pseudo rapidity $\\eta = - \\ln{\\tan{\\frac{\\theta}{2}}}$ is primarily used in the ATLAS analyses\n",
    "     - Variable names: `lep1_eta`, `lep2_eta`, `lep3_eta`, `lep4_eta`\n",
    "- Lepton energy\n",
    "     - The energy of the leptons reconstructed from the calorimeter entries\n",
    "     - Variable name: `lep1_e`, `lep2_e`, `lep3_e`, `lep4_e`\n",
    "- Lepton PDG-ID\n",
    "     - The lepton type is classified by a n umber given by the Particle-Data-Group.<br>\n",
    "     The lepton types are PDG-ID$(e)=11$, PDG-ID$(\\mu)=13$ and PDG-ID$(\\tau)=15$\n",
    "     - Variable name: `lep1_pdgId`, `lep2_pdgId`, `lep3_pdgId`, `lep4_pdgId`\n",
    "- Lepton charge\n",
    "     - The charge of the given lepton reconstructed by the lepton track\n",
    "     - Variable name: `lep1_charge`, `lep2_charge`, `lep3_charge`, `lep4_charge`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7d2bda",
   "metadata": {
    "id": "9a7d2bda"
   },
   "source": [
    "### Event Pre-Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55190edc",
   "metadata": {},
   "source": [
    "Before we start with the pre-selection of the input data check the number of events per process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859620c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all processes\n",
    "for sample in sample_list_signal + sample_list_background:\n",
    "    # Sum over the weights is equal to the number of expected events\n",
    "    n_events = sum(no_selection_data_frames[sample]['totalWeight'])\n",
    "    # Number of raw simulation events\n",
    "    n_events_raw = len(no_selection_data_frames[sample]['totalWeight'])\n",
    "    print(f'{sample}: {round(n_events, 2)}; {n_events_raw} (raw)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facd6d51",
   "metadata": {
    "id": "facd6d51"
   },
   "source": [
    "Although the final selection of the data is to be performed on the basis of a DNN, a rough pre-selection of the data is still useful.\n",
    "For this purpose, selection criteria are defined, which return either true or false based on the event kinematics and thus decide whether the respective event is kept or discarded.\n",
    "Suitable criteria for this analysis are very basic selections that must be clearly fulfilled by H->ZZ->llll processes.\n",
    "\n",
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Find baseline selection criteria that reduce the background while keeping almost all Higgs events. \n",
    "</font>\n",
    "\n",
    "\n",
    "Hint: What lepton types and charges are expected in the final state?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa57853",
   "metadata": {
    "id": "ffa57853"
   },
   "outputs": [],
   "source": [
    "def cut_dummy(lep1_e):\n",
    "    \"\"\"This is an dummy cut which all events pass with a leading lepton energy higher than 50 GeV\"\"\"\n",
    "    return lep1_e > 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fc29db",
   "metadata": {
    "id": "13fc29db"
   },
   "outputs": [],
   "source": [
    "# Create a copy of the original data frame to investigate later\n",
    "data_frames = no_selection_data_frames.copy()\n",
    "\n",
    "# Apply the chosen selection criteria\n",
    "for sample in sample_list_signal + sample_list_background + sample_list_measured:\n",
    "    # Dummy selection\n",
    "    dummy_selection = np.vectorize(cut_dummy)(data_frames[sample].lep1_e)\n",
    "    data_frames[sample] = data_frames[sample][dummy_selection]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe35da1e",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Check wether your selection criteria have the required effects \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cff76d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all processes\n",
    "for sample in sample_list_signal + sample_list_background:\n",
    "    # Sum over the weights is equal to the number of expected events\n",
    "    n_events = \n",
    "    # Number of raw simulation events\n",
    "    n_events_raw = \n",
    "    print(f'{sample}: {round(n_events, 2)}; {n_events_raw} (raw)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bde2f3",
   "metadata": {},
   "source": [
    "If you are happy with you baseline selection continue with the investigation of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94202a44",
   "metadata": {
    "id": "94202a44"
   },
   "source": [
    "### Data Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e11cc8e",
   "metadata": {
    "id": "7e11cc8e"
   },
   "source": [
    "Before one can decide which variables are suitable for training, one must first get a feel for the input variables.\n",
    "For this purpose, the input samples are merged into a set of signal events and a set of background events. Afterwards, the behavior of signal and background can be studied in multiple variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591b7e64",
   "metadata": {
    "id": "591b7e64"
   },
   "outputs": [],
   "source": [
    "# Merge the signal and background data frames\n",
    "def merge_data_frames(sample_list, data_frames_dic):\n",
    "    for sample in sample_list:\n",
    "        if sample == sample_list[0]:\n",
    "            output_data_frame = data_frames_dic[sample]\n",
    "        else:\n",
    "            output_data_frame = pd.concat([output_data_frame, data_frames_dic[sample]], axis=0)\n",
    "    return output_data_frame\n",
    "\n",
    "data_frame_signal = merge_data_frames(sample_list_signal, data_frames)\n",
    "data_frame_background = merge_data_frames(sample_list_background, data_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2906682a",
   "metadata": {
    "id": "2906682a"
   },
   "source": [
    "The function common.plot_hist(variable, data_frame_1, data_frame_2) plots the given variable of the two datasets.\n",
    "The variable must be a dictionary containing atleast the variable to plot. Additionally one can also specify the binning (list or numpy array) and the xlabel. The created histogram is automatically saved in the plots directory<br>\n",
    "\n",
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Which variable is the most discriminant? Which variables seem not discriminant at all? \n",
    "</font>\n",
    "\n",
    "An example for the transverse momnetum of the leading lepton is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae8494b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8ae8494b",
    "outputId": "7451147f-06aa-4645-d231-e88f13492df4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# leading lepton pt\n",
    "var_lep1_pt = {'variable': 'lep1_pt',\n",
    "               'binning': np.linspace(0, 300, 50),\n",
    "               'xlabel': '$p_T$ (lep 1) [GeV]'}\n",
    "\n",
    "common.plot_hist(var_lep1_pt, data_frames)\n",
    "common.plot_normed_signal_vs_background(var_lep1_pt, data_frame_signal, data_frame_background)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38a5a5e",
   "metadata": {
    "id": "a38a5a5e"
   },
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "What is the purity in signal events for the given data?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7415e53",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b7415e53",
    "outputId": "e7d7a213-8fad-4519-be33-0f0381fd1706",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ceb9ac87",
   "metadata": {
    "id": "ceb9ac87"
   },
   "source": [
    "As one could already see, the number of simulated raw events is significantly higher than the weighted number of expected events. The contribution of a simulated event to the final prediction is thus given by the respective event weight.\n",
    "\n",
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "How many raw events are included in each process simulation and what the corresponding total prediction? What the is minimal, median, maximal event weight of each process?\n",
    "</font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e62a57e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for sample in sample_list_signal + sample_list_background:\n",
    "    print(f'{sample}:')\n",
    "    n_events = \n",
    "    n_events_raw = \n",
    "    min_weight = \n",
    "    med_weight = \n",
    "    max_weight = \n",
    "    print(f'  Raw events:      {n_events_raw}')\n",
    "    print(f'  Prediction:      {round(n_events, 2)}')\n",
    "    print(f'  Minimal weight:  {min_weight}')\n",
    "    print(f'  Median weight:   {med_weight}')\n",
    "    print(f'  Maximal weight:  {max_weight}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6711866a",
   "metadata": {},
   "source": [
    "The different processes were simulated with different accuracy. To model 267 $llll$ events more than half a million raw events are used but 96 Z$\\rightarrow ll$ is modelled by only 500 events.\n",
    "\n",
    "Furthermore, it seems that the event weights even go into the negative range. Negative weighted events are produced to compensate overshooting predictions in certain kinematic areas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6UOuLmyqGkVq",
   "metadata": {
    "id": "6UOuLmyqGkVq"
   },
   "source": [
    "## Choice of the Training Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18d9d5d",
   "metadata": {},
   "source": [
    "In this part of the notebook we will create and train some simple neural networks. The goal is to find the right setup for training, as this is a basic requirement for successful training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s3wju-KVRZmk",
   "metadata": {
    "id": "s3wju-KVRZmk"
   },
   "source": [
    "### How to Create and Train a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YDoe0Nba4osz",
   "metadata": {
    "id": "YDoe0Nba4osz"
   },
   "source": [
    "Lets start with a very simple training on the transverse momentum of the leptons. To speed up the training not all processes are considered for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZLhbQ-gYKIa4",
   "metadata": {
    "id": "ZLhbQ-gYKIa4"
   },
   "outputs": [],
   "source": [
    "# The training input variables\n",
    "training_variables = ['lep1_pt', 'lep2_pt', 'lep3_pt', 'lep4_pt']\n",
    "# Use only a subset of the full data\n",
    "sample_subset_signal = ['ggH125_ZZ4lep']\n",
    "sample_subset_background = ['llll', 'Zee', 'Zmumu']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GkPQ0w0h9CjZ",
   "metadata": {
    "id": "GkPQ0w0h9CjZ"
   },
   "source": [
    "Extract the training variables, the event weights, and the classification of the events. The classification is 0 for background processes and 1 for Higgs events. \n",
    "To further speed up the setup test only use 40% of the available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JBnQRQ1A7U-F",
   "metadata": {
    "id": "JBnQRQ1A7U-F"
   },
   "outputs": [],
   "source": [
    "values, weights, classification = common.get_dnn_input(data_frames, training_variables, sample_subset_signal, sample_subset_background, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa10b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show values of 5 random events\n",
    "random_idx = [1841, 11852, 63217, 78357, 131697]\n",
    "# Training variables\n",
    "print('Training values:')\n",
    "print(values[random_idx])\n",
    "# Event weights\n",
    "print('Event weights:')\n",
    "print(weights[random_idx])\n",
    "# Classification\n",
    "print('Classification:')\n",
    "print(classification[random_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65a9792",
   "metadata": {},
   "source": [
    "Tensorflow is used to create and train neural networks. For this purpose, the required module is imported and the existing training values and classifications are transformed into a Tensorflow dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QnyJWmRnLNSK",
   "metadata": {
    "id": "QnyJWmRnLNSK"
   },
   "outputs": [],
   "source": [
    "# Import the tensorflow module to create a DNN\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S8WnIC0VToQw",
   "metadata": {
    "id": "S8WnIC0VToQw"
   },
   "outputs": [],
   "source": [
    "# Convert the data to tensorflow datasets\n",
    "train_data = Dataset.from_tensor_slices((values, classification))\n",
    "train_data = train_data.shuffle(len(values), seed=random_state)\n",
    "# Set the batch size\n",
    "train_data = train_data.batch(128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257c0909",
   "metadata": {},
   "source": [
    "So lets create a neural network consisting of several layers.\n",
    "\n",
    "A full list of layer types can be found here https://www.tensorflow.org/api_docs/python/tf/keras/layers <br>\n",
    "Some important examples for layers are:\n",
    "- Normalization: Shift and scale the training input to have the center at 0 and a standard deviation of 1 \n",
    "- Dense: a densely connected NN layer\n",
    "- Dropout: randomly sets input to 0. Can decrease overtraining\n",
    "\n",
    "The neurons of each layer are activated by the so called activation function. <br>\n",
    "A full list of provided activation functions can be found here https://www.tensorflow.org/api_docs/python/tf/keras/activations <br>\n",
    "Some examples are:\n",
    "- Linear: $linear(x) = x$\n",
    "- Relu: $relu(x) = max(0, x)$\n",
    "- Exponential: $exponential(x) = e^x$\n",
    "- Sigmoid: $sigmoid(x) = 1 / (1 + e^{-x})$\n",
    "\n",
    "The activation of i-th node in the k-th layer $a_i^{(k)}$ is given by the sum of the activations of the previous layer $a_j^{(k-1)}$ multiplied by the trainings weights $w_{j; i}^{(k)}$ plus a bias $b_i^{(k)}$. For this sum the activation function $f_{activ}$ is called resulting in the activation of the current node.<br>\n",
    "$a_i^{(k)} = f_{activ}(\\sum_j w_{j; i}^{(k)} \\cdot a_j^{k-1} + b_i^{(k)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67b27dd",
   "metadata": {},
   "source": [
    "Let's create following model:\n",
    "<div>\n",
    "<img src='figures/simple_model.png' width='900'/>\n",
    "</div>\n",
    "\n",
    "- The input variables are scaled and shifted to have the mean of 0 and the standard deviation of 1. This normalization improves the convergence of the training process.\n",
    "- Use two dense layers with 60 neurons each. The neurons are activated with the relu function.\n",
    "- To classify background and signal the last layer should only consist of one neuron which has an activation between 0 and 1. The activation between 0 and 1 if given by the sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dba1f2d",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "How many parameters do you expect for this model?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4025b1",
   "metadata": {},
   "source": [
    "Create a normalization layer and adapt it to the training values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_QXqaSb3CYrs",
   "metadata": {
    "id": "_QXqaSb3CYrs"
   },
   "outputs": [],
   "source": [
    "# Normalization layer\n",
    "normalization_layer = tf.keras.layers.Normalization(name='Input_normalization')\n",
    "normalization_layer.adapt(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8074701d",
   "metadata": {},
   "source": [
    "Create a list of model layers and built the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a4MIvWLNn_",
   "metadata": {
    "id": "49a4MIvWLNn_"
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "model_layers = [\n",
    "    normalization_layer,\n",
    "    tf.keras.layers.Dense(60, activation='relu', name='Hidden_layer_1'),\n",
    "    tf.keras.layers.Dense(60, activation='relu', name='Hidden_layer_2'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid', name='Output_layer'),\n",
    "]\n",
    "model = tf.keras.models.Sequential(model_layers, name='simple_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b3d324",
   "metadata": {},
   "source": [
    "Before we start the training of the model lets check the shape and the number of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CJkzGU4giA6_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CJkzGU4giA6_",
    "outputId": "37497ac1-9ecf-49d0-cd5a-cce305d8d1e2"
   },
   "outputs": [],
   "source": [
    "# Display the model's architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ovNG9YVDMD-0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 533
    },
    "id": "ovNG9YVDMD-0",
    "outputId": "fdc7116e-c111-4079-a620-0084d050e06d"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True, show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c3340d",
   "metadata": {},
   "source": [
    "For the training we have to choose an optimization criteria, the loss function. For supervised learning, the loss function returns a value for the divergence between model prediction and the true value. The loss value is high for bad agreement and low for good agreement. Thus, training the model means minimizing the loss function.\n",
    "The seperartion of signal and background is binary classification problem and thus the binary cross-entropy is used to calculate the loss.\n",
    "The binary cross-entropy is given by:<br>\n",
    "$H = -\\frac{1}{N} \\sum_i^N (y_i^{true} log(y_i^{predict}) + (1 - y_i^{true}) log(1 - y_i^{predict}))$\n",
    "\n",
    "As you can see the formula distinguishes signal events with $y_i^{true} = 1$ and background events with $y_i^{true} = 0$. In the following figure you can see the cross entropy for a signal and a background event for different prediction values. The closer the prediction value is to the true classification the smaller the crossentropy is. This is exactly the behavior we need for our loss function, and thus training the neural network means minimizing the mean cross-entropy of all training events.\n",
    "<div>\n",
    "<img src='figures/binary_cross_entropy.png' width='700'/>\n",
    "</div>\n",
    "\n",
    "If you are still wondering why we use the logarithms and why it is called entropy you are encouraged to check out some information theory. The origin of this cross-entropy can be found in the entropy definition in information theory. The entropy of the true distribution is always smaller than the cross-entropy of its estimator. Therefore, minimizing the cross-entropy is equivalent to becomming closer to the true distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qdunQSwiLNzk",
   "metadata": {
    "id": "qdunQSwiLNzk"
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c421ceb",
   "metadata": {},
   "source": [
    "The loss function itself seems not complicated but keep in mind in our example it depends on several thousand parameters of the neural network. Thus, finding the correct parameter combination to minimize the loss is a quite complex problem. The Adam optimizer we will use for the training is a stochastic gradient descent method.\n",
    "\n",
    "When we converted the training a batch size was chosen `train_data = train_data.batch(128)` which are subsets of the given data. While training the optimizer computes the gradient descent und updates the parameters for each batch.\n",
    "After updating the parameters for all batches one after the other, this process can be started again from the beginning. A complete run over all training data is called an epoch. After several epochs, the parameters have been adjusted several times for all training data and hopefully a good combination of neural network parameters has been found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5da3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "adam_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0002, beta_1=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6ca2b4",
   "metadata": {},
   "source": [
    "Configure the model for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K55rSNabMPMb",
   "metadata": {
    "id": "K55rSNabMPMb"
   },
   "outputs": [],
   "source": [
    "# Compilation\n",
    "model.compile(optimizer=adam_optimizer, loss=loss_fn, metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da12bbc",
   "metadata": {},
   "source": [
    "Lets train the model for 5 epochs and store the training history in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0SEg6o8xMPVH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0SEg6o8xMPVH",
    "outputId": "b208f5b1-2bfa-4f96-92fc-c9f5ffa34948",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_data, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978acc56",
   "metadata": {},
   "source": [
    "Lets visualize the training progress of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aTlPvPpMPfD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "2aTlPvPpMPfD",
    "outputId": "265be094-c99d-4a78-b90f-636c38c63e27"
   },
   "outputs": [],
   "source": [
    "# Plot the training history\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "ax.plot(history.history['loss'])\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('loss')\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b0046f",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Do you think the training was done after 5 epochs? \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c5b329",
   "metadata": {},
   "source": [
    "The model is trained, so the time has come to use it. Let's compare the true classification of some random events and the prediction the model gives for these events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B84a0pgfWGOK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B84a0pgfWGOK",
    "outputId": "ec06e7f9-7a0f-46b4-fd16-906f629de59e"
   },
   "outputs": [],
   "source": [
    "# Choose some random events\n",
    "random_idx = [1841, 11852, 63217, 78357, 131697]\n",
    "print('Classification:')\n",
    "print(classification[random_idx])\n",
    "print('Prediction')\n",
    "print(model.predict(values[random_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c00d5b9",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Get the prediction for all events and use <code>common.plot_dnn_output(...)</code> to plot the result.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EMhKpy_gIOXL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EMhKpy_gIOXL",
    "outputId": "7313ce3c-6351-4002-f82a-7ee1cb4b4d6b"
   },
   "outputs": [],
   "source": [
    "# Apply the model for all values\n",
    "prediction = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MTd_tOKCIPJy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "id": "MTd_tOKCIPJy",
    "outputId": "1be5e574-d0d5-47f4-a218-90759a356dc9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the model output\n",
    "common.plot_dnn_output(prediction, classification)\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697eb973",
   "metadata": {},
   "source": [
    "Use `common.apply_dnn_model(...)` to apply the model for all samples in `data_frames` and add the classification to the data frame. Afterwards the prediction can be plotted just as the other kinematic variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lD6O2_YOB0Vv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lD6O2_YOB0Vv",
    "outputId": "08b6d534-3139-4a19-9210-1a0559d31bfb"
   },
   "outputs": [],
   "source": [
    "# Apply the model\n",
    "data_frames_apply_dnn = common.apply_dnn_model(model, data_frames, training_variables, sample_subset_signal + sample_subset_background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eDpIAGxTBpUe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "eDpIAGxTBpUe",
    "outputId": "250fa7f0-3b1d-422f-dbc6-61857c5f5556"
   },
   "outputs": [],
   "source": [
    "model_prediction = {'variable': 'model_prediction',\n",
    "                    'binning': np.linspace(0, 1, 50),\n",
    "                    'xlabel': 'prediction'}\n",
    "common.plot_hist(model_prediction, data_frames_apply_dnn, show_data=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ew01qIqSRoar",
   "metadata": {
    "id": "ew01qIqSRoar"
   },
   "source": [
    "### Validation Data and Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6rodq6-jQWR5",
   "metadata": {
    "id": "6rodq6-jQWR5"
   },
   "source": [
    "How long do we have to train? The right amount of training is very important for the final performance of the network. If the training was too short the model parameters are poorly adapted to the underlying concepts and the model performance is bad. This is called undertraining. If the training was too long the model will start to learn the training data by heart. This overtraining will lead to a very godd performance at the training data but bad performance on unseen data.\n",
    "\n",
    "Thus, to test the performance of the model the model has to be applied on unseen data. After each epoch the model is the model is applied on validation data not used for training. If the classification of the validation data has improved for the current epoch the model performance is still improving. If the performance on the validation data does not improve anymore the training can be stopped.\n",
    "\n",
    "<div>\n",
    "<img src='figures/over_and_under_training.png' width='700'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kML3bli6SUfW",
   "metadata": {
    "id": "kML3bli6SUfW"
   },
   "outputs": [],
   "source": [
    "# Import function to split data into train and test data\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hQb2GTSGSg0o",
   "metadata": {
    "id": "hQb2GTSGSg0o"
   },
   "source": [
    "Use again 40% of the available data for training and additionally 20% to validate the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KLZpYpYF_Lai",
   "metadata": {
    "id": "KLZpYpYF_Lai"
   },
   "outputs": [],
   "source": [
    "# Extract the values, weights and classification of 60% of the data\n",
    "values, weights, classification = common.get_dnn_input(data_frames, training_variables, sample_list_signal, sample_list_background, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5sr3kiRq_Nx8",
   "metadata": {
    "id": "5sr3kiRq_Nx8"
   },
   "outputs": [],
   "source": [
    "# Split into train and validation data\n",
    "train_values, val_values, train_classification, val_classification = train_test_split(values, classification, test_size=1/3, random_state=random_state)\n",
    "train_weights, val_weights = train_test_split(weights, classification, test_size=1/3, random_state=random_state)[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c86e967",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Let's follow the same strategy as before:\n",
    "    \n",
    "- Create tensorflow datasets for training and validation data with 128 events per batch\n",
    "- Recreate and adapt the normalization layer\n",
    "- Recreate the tensorflow model with the same number of layers and nodes per layer\n",
    "- Compile the model\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PhQihlcESpEO",
   "metadata": {
    "id": "PhQihlcESpEO"
   },
   "outputs": [],
   "source": [
    "# Convert the data to tensorflow datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zhDgmhgMSCEQ",
   "metadata": {
    "id": "zhDgmhgMSCEQ"
   },
   "outputs": [],
   "source": [
    "# Normalization layer\n",
    "\n",
    "# Create a simple NN\n",
    "model_layers = [\n",
    "]\n",
    "model = tf.keras.models.Sequential(model_layers)\n",
    "# Compile model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fc4cc6",
   "metadata": {},
   "source": [
    "So how to stop the training if the perfomance does not improve anymore?\n",
    "\n",
    "The answer is early stopping. With early stopping you set a value which should be monitored, in our case the loss on the validation data `val_loss`. Since there can be fluctuations in the tested model performance, it is recommended to use a certain patience after which the training should be stopped. If we set `patience=5` the training is stopped if the `va_loss` has not improved for 5 epochs. Since the model performance has potentionally decreased during this 5 epochs set `restore_best_weights=True` to restore the model state with the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wyTjuAZHThGI",
   "metadata": {
    "id": "wyTjuAZHThGI"
   },
   "outputs": [],
   "source": [
    "# Early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HpQzXeAgSCZr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HpQzXeAgSCZr",
    "outputId": "aa13403f-a297-40a4-fb1b-450b4004040b"
   },
   "outputs": [],
   "source": [
    "# Train model with early stopping for the validation data performance\n",
    "history = model.fit(train_data, validation_data=val_data, callbacks=[early_stopping], epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8cf6dc",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Plot the training loss (<code>history.history['loss']</code>) and validation loss (<code>history.history['val_loss']</code>) of the training history. Describe what behavior you can observe for each performance trend.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uaCAcUKUWMuh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 543
    },
    "id": "uaCAcUKUWMuh",
    "outputId": "8d15f192-debc-4338-b243-6a2c310b9d16",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the training history\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('loss')\n",
    "ax.legend()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1693cf8",
   "metadata": {},
   "source": [
    "The model itself has already an implemented evaluation function. When a tensorflow set is provided it returns the loss and accuracy on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-2pKN3BvCd26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-2pKN3BvCd26",
    "outputId": "b91378e3-78d0-411b-9b24-3562da837410",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate model on training\n",
    "model_train_evaluation = model.evaluate(train_data)\n",
    "\n",
    "print(f'train loss = {round(model_train_evaluation[0], 5)}\\ttrain accuracy = {round(model_train_evaluation[1], 5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e034f48",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Evaluate the model on validation data and compare the results to the validation on training data.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fff1f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on validation data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c46efdb",
   "metadata": {},
   "source": [
    "Now lets apply the model on the train and validation data and plot the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Tq0pWJFBWM5x",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tq0pWJFBWM5x",
    "outputId": "d8514d14-7782-421e-9215-5aecfaae62f3"
   },
   "outputs": [],
   "source": [
    "# Apply the model for training and validation values\n",
    "train_prediction = model.predict(train_values)\n",
    "val_prediction = model.predict(val_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1WBms0WNCc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "id": "5d1WBms0WNCc",
    "outputId": "0ee6e8ae-e53d-4472-a3b9-77ae0dcad6b7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the model output\n",
    "common.plot_dnn_output(train_prediction, train_classification, val_prediction, val_classification)\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d4156a",
   "metadata": {},
   "source": [
    "As you can see the classification by the model on traning and validation data is very consistent. This is great :)<br>\n",
    "If we would see a significant difference in train and validation classification this would be a clear sign for overtraining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abad074e",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Use <code>common.apply_dnn_model(...)</code> to apply the model for all samples in <code>data_frames</code> and plot the classification.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bzgiLWMWWGmw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bzgiLWMWWGmw",
    "outputId": "3f8d6b0d-2d86-41d8-c85e-4890f7189156"
   },
   "outputs": [],
   "source": [
    "# Apply the model\n",
    "data_frames_apply_dnn = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VgBXHg_6bAP7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "VgBXHg_6bAP7",
    "outputId": "e31607c8-8c43-4b39-8dba-c89c3402acf5"
   },
   "outputs": [],
   "source": [
    "model_prediction = {'variable': 'model_prediction',\n",
    "                    'binning': np.linspace(0, 1, 50),\n",
    "                    'xlabel': 'prediction'}\n",
    "common.plot_hist(model_prediction, data_frames_apply_dnn, show_data=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RRuPU0vhF1Ch",
   "metadata": {
    "id": "RRuPU0vhF1Ch"
   },
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "The llll events are mostly classified as background and the Higgs events tend to the signal classification. However, the classification of Zee and Zmumu events seems to be quite random. What could be the reason for this?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "txeJepxIIPYh",
   "metadata": {
    "id": "txeJepxIIPYh"
   },
   "source": [
    "### Event Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46db027e",
   "metadata": {},
   "source": [
    "The simulated events are multiplied by weight factors for the final prediction of measured data. Thus, events with large weights are more important for the prediction than events with small weights. In order to take this into account for the training, the event weights `w_i` can be included in the calculation of the loss:<br>\n",
    "$H = -\\frac{1}{N} \\sum_i^N w_i (y_i^{true} log(y_i^{predict}) + (1 - y_i^{true}) log(1 - y_i^{predict}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7d5a3c",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "And another time follow the same strategy as before:\n",
    "    \n",
    "- Create tensorflow datasets for training and validation data with 128 events per batch. But this time also include the event weights\n",
    "- Recreate and adapt the normalization layer\n",
    "- Recreate the tensorflow model with the same number of layers and nodes per layer\n",
    "- Compile the model\n",
    "- Train the model\n",
    "- Plot the classification and do the evaluation\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jM5nQz3dsN3t",
   "metadata": {
    "id": "jM5nQz3dsN3t"
   },
   "outputs": [],
   "source": [
    "# Convert the data to tensorflow datasets\n",
    "train_data = Dataset.from_tensor_slices((train_values, train_classification, train_weights))\n",
    "train_data = train_data.shuffle(len(train_data), seed=random_state)\n",
    "train_data = train_data.batch(128)\n",
    "val_data = Dataset.from_tensor_slices((val_values, val_classification, val_weights))\n",
    "val_data = val_data.shuffle(len(val_data), seed=random_state)\n",
    "val_data = val_data.batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada5KISkbAah",
   "metadata": {
    "id": "ada5KISkbAah"
   },
   "outputs": [],
   "source": [
    "# Normalization layer\n",
    "\n",
    "# Create a simple NN\n",
    "model_layers = [\n",
    "]\n",
    "model = tf.keras.models.Sequential(model_layers)\n",
    "# Compile model now with the weighted metric\n",
    "model.compile(optimizer=adam_optimizer, loss=loss_fn, weighted_metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GPnLZ6V0bAjX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GPnLZ6V0bAjX",
    "outputId": "765d4daa-c23b-400b-fc7d-018733ef46e1"
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Iyh3S_PZR91J",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "id": "Iyh3S_PZR91J",
    "outputId": "771ec678-30f1-4c79-edb1-b46f1afb7b23"
   },
   "outputs": [],
   "source": [
    "# Plot the training history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VcSLHpwBbAtP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "id": "VcSLHpwBbAtP",
    "outputId": "1f900781-0616-4cff-8c6c-0bfb89f95a87"
   },
   "outputs": [],
   "source": [
    "# Apply the model for training and validation values\n",
    "\n",
    "# Plot the model output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dG_-a34VIKm0",
   "metadata": {
    "id": "dG_-a34VIKm0"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on training and validation data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fCt5AGQrbA1t",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 777
    },
    "id": "fCt5AGQrbA1t",
    "outputId": "0ce71a54-636e-4b49-e0d9-b227b3f7bbbb"
   },
   "outputs": [],
   "source": [
    "# Apply the model\n",
    "data_frames_apply_dnn = common.apply_dnn_model(model, data_frames, training_variables, sample_subset_signal + sample_subset_background)\n",
    "model_prediction = {'variable': 'model_prediction',\n",
    "                    'binning': np.linspace(0, 1, 50),\n",
    "                    'xlabel': 'prediction'}\n",
    "common.plot_hist(model_prediction, data_frames_apply_dnn, show_data=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860c3532",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "What differences can you see on the loss, accuarcy and classification distribution? Explain these differences.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe1d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the event weights\n",
    "mean_signal_weight = weights[classification > 0.5].mean()\n",
    "mean_background_weight = weights[classification < 0.5].mean()\n",
    "print(f'The mean weight for signal events is {mean_signal_weight} and for background events {mean_background_weight}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66701ee4",
   "metadata": {},
   "source": [
    "#### Reweight the weights of the simulated data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d212439d",
   "metadata": {},
   "source": [
    "Introducing the event weights caused the problem that the overall background dominates the training. With the event weights of the simulated data the loss is already quite low if all data is consequently classified close to background.<br>\n",
    "$H = -\\frac{1}{N} \\sum_i^N w_i (y_i^{true} log(y_i^{predict}) + (1 - y_i^{true}) log(1 - y_i^{predict}))$\n",
    "\n",
    "To solve this problem we should reweight the weights to have the same effect on training for signal and background events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3753c4ab",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Write a function that reweights the weights:<br>\n",
    "1. Take the absolute value of the weight to not run into problems with negative weights\n",
    "2. Split the weights into signal and background weights (by setting the other weights in the array to zero)\n",
    "3. Scale the signal weights to have the total sum as the background weights\n",
    "4. Merge the background and scaled signal weights\n",
    "5. Scale all weights to have a mean weight of 1\n",
    "</font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MoHxpNYmQSiL",
   "metadata": {
    "id": "MoHxpNYmQSiL"
   },
   "outputs": [],
   "source": [
    "def reweight_weights(weights, classification):\n",
    "    # Take the absolute value of the weight\n",
    "    weights_abs = \n",
    "    # Split in signal and background weights\n",
    "    weights_signal = weights_abs*classification\n",
    "    weights_background = weights_abs*(1 - classification)\n",
    "    # Scale the signal events\n",
    "    weights_signal_scaled = \n",
    "    # Merge the signal and background events\n",
    "    weights_reweighted = weights_background + weights_signal_scaled\n",
    "    # Normalize mean weight to one\n",
    "    weights_reweighted =\n",
    "    return weights_reweighted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2035fe",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Test your reweighting function. The mean of all weights should be 1 and the sum of signal weights and background weights should be equal.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f192df",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_reweighted = reweight_weights(weights, classification)\n",
    "signal_weights_reweighted = weights_reweighted[classification > 0.5]\n",
    "background_weights_reweighted = weights_reweighted[classification < 0.5]\n",
    "\n",
    "print(f'Mean weight: {}')\n",
    "print(f'Signal weight sum: {}')\n",
    "print(f'Background weight sum: {}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0831d1",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Run the training and plot the classification and evaluate the model.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5mTwdrlOQSzZ",
   "metadata": {
    "id": "5mTwdrlOQSzZ"
   },
   "outputs": [],
   "source": [
    "# Get reweighted weights\n",
    "train_weights_reweighted = reweight_weights(train_weights, train_classification)\n",
    "val_weights_reweighted = reweight_weights(val_weights, val_classification)\n",
    "\n",
    "# Convert the data to tensorflow datasets\n",
    "train_data = Dataset.from_tensor_slices((train_values, train_classification, train_weights_reweighted))\n",
    "train_data = train_data.shuffle(len(train_data), seed=random_state)\n",
    "train_data = train_data.batch(128)\n",
    "val_data = Dataset.from_tensor_slices((val_values, val_classification, val_weights_reweighted))\n",
    "val_data = val_data.shuffle(len(val_data), seed=random_state)\n",
    "val_data = val_data.batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dBrso_7_QTDe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dBrso_7_QTDe",
    "outputId": "1c70bb48-1113-41da-8c86-351df614ea94"
   },
   "outputs": [],
   "source": [
    "# Normalization layer\n",
    "normalization_layer = tf.keras.layers.Normalization()\n",
    "normalization_layer.adapt(train_values)\n",
    "# Create a simple NN\n",
    "model_layers = [\n",
    "    normalization_layer,\n",
    "    tf.keras.layers.Dense(60, activation='relu'),\n",
    "    tf.keras.layers.Dense(60, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "]\n",
    "model = tf.keras.models.Sequential(model_layers)\n",
    "# Compile model\n",
    "model.compile(optimizer=adam_optimizer, loss=loss_fn, weighted_metrics=['binary_accuracy'])\n",
    "# Train model\n",
    "history = model.fit(train_data, validation_data=val_data, callbacks=[early_stopping], epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pOnxDoVXQTT4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "pOnxDoVXQTT4",
    "outputId": "12eba90b-923e-4328-e304-00253e772ae1"
   },
   "outputs": [],
   "source": [
    "# Plot the training history\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "ax.plot(history.history['loss'], label='training')\n",
    "ax.plot(history.history['val_loss'], label='validation')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('loss')\n",
    "ax.legend()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ap3ul7SQTzJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "id": "7ap3ul7SQTzJ",
    "outputId": "fc47ce20-fe1f-4a32-80aa-1e525ce686a7"
   },
   "outputs": [],
   "source": [
    "# Apply the model for training and validation values\n",
    "\n",
    "# Plot the model output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mPQ9yQ7TSMnh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 777
    },
    "id": "mPQ9yQ7TSMnh",
    "outputId": "c9611a6b-ccea-4a06-b32f-540d34a170e8"
   },
   "outputs": [],
   "source": [
    "# Apply the model\n",
    "data_frames_apply_dnn = common.apply_dnn_model(model, data_frames, training_variables, sample_subset_signal + sample_subset_background)\n",
    "model_prediction = {'variable': 'model_prediction',\n",
    "                    'binning': np.linspace(0, 1, 50),\n",
    "                    'xlabel': 'prediction'}\n",
    "common.plot_hist(model_prediction, data_frames_apply_dnn, show_data=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefa7d17",
   "metadata": {
    "id": "nSH3fPCBSM6j"
   },
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Compare this classification plot with the one resulting from a training without event weights.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DnaOe4OGlaL9",
   "metadata": {
    "id": "DnaOe4OGlaL9"
   },
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aa7519",
   "metadata": {},
   "source": [
    "If you have a look on the training history you see fluctuations in the validation loss. Furthermore, the validation dataset also has a limited size, making it potentially not completely representative for validation.\n",
    "\n",
    "So how should one evaluate the performance of a model and compare two models?\n",
    "\n",
    "A commonly used method to evaluate the model performance is k-fold.\n",
    "The training data is split several times with non-overlapping validation sets. On each split a model is trained and validated on the corresponding validation data. This results into several independently trained models with same size and setup validated on different datasets. Thus, one is able to calculate the mean performance of the resulting models.\n",
    "<div>\n",
    "<img src='figures/kFold.png' width='500'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p2YYk5F9SNJl",
   "metadata": {
    "id": "p2YYk5F9SNJl"
   },
   "outputs": [],
   "source": [
    "# import the kFold module\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6624d93",
   "metadata": {},
   "source": [
    "Use kFold to split the data 3 times in 2/3 training and 1/3 validation data each randomly shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dI3Zp2xXSPri",
   "metadata": {
    "id": "dI3Zp2xXSPri"
   },
   "outputs": [],
   "source": [
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d33e0c",
   "metadata": {},
   "source": [
    "Now use kFold to train several models in a for loop. In each loop you have to create a new model and its setup.\n",
    "\n",
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Fill the missing parts in the for loop:\n",
    "- Reweight the weights\n",
    "- Convert the values and classification into tensorflow datasets\n",
    "- Create a model with normalization layer and 2 hidden layers with 60 nodes each\n",
    "- Compile the model\n",
    "- Train the model with early stopping\n",
    "- Evaluate the model on the training and evaluation data\n",
    "</font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "--Jm5SPaSQFm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "--Jm5SPaSQFm",
    "outputId": "73cef139-4304-4936-ff12-025a66482277"
   },
   "outputs": [],
   "source": [
    "# Store the models and their training history\n",
    "kfold_history = []\n",
    "kfold_model = []\n",
    "# Store the evaluation on training and validation data\n",
    "kfold_train_eval_loss = []\n",
    "kfold_train_eval_acc = []\n",
    "kfold_val_eval_loss = []\n",
    "kfold_val_eval_acc = []\n",
    "split_idx = 1\n",
    "for train_indices, val_indices in kfold.split(values):\n",
    "    print(f'Use fold {split_idx}')\n",
    "    split_idx += 1\n",
    "    # Get train and validation data \n",
    "    train_values = values[train_indices]\n",
    "    train_classification = classification[train_indices]\n",
    "    train_weights = weights[train_indices]\n",
    "    val_values = values[val_indices]\n",
    "    val_classification = classification[val_indices]\n",
    "    val_weights = weights[val_indices]\n",
    "    # Get reweighted weights\n",
    "    train_weights_reweighted = \n",
    "    val_weights_reweighted = \n",
    "    # Get train and validation datasets \n",
    "\n",
    "\n",
    "    # Normalization layer\n",
    "    \n",
    "    # Create a simple NN\n",
    "    model_layers = [\n",
    "    ]\n",
    "    model = tf.keras.models.Sequential(model_layers)\n",
    "    # Compile model\n",
    "    \n",
    "\n",
    "    # Train model\n",
    "    history = \n",
    "\n",
    "    # Append to list\n",
    "    kfold_history.append(history)\n",
    "    kfold_model.append(model)\n",
    "\n",
    "    # Evaluate model on training and validation data\n",
    "    model_train_evaluation = \n",
    "    model_val_evaluation = \n",
    "    kfold_train_eval_loss.append(model_train_evaluation[0])\n",
    "    kfold_train_eval_acc.append(model_train_evaluation[1])\n",
    "    kfold_val_eval_loss.append(model_val_evaluation[0])\n",
    "    kfold_val_eval_acc.append(model_val_evaluation[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a87fdf8",
   "metadata": {},
   "source": [
    "Lets plot the training history of the three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L1pKiwRvrcok",
   "metadata": {
    "id": "L1pKiwRvrcok",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the training history\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "color_list = ['r', 'g', 'b']\n",
    "for k_fold_idx, (history, color) in enumerate(zip(kfold_history, color_list)):\n",
    "  ax.plot(history.history['loss'], color=color, label=f'{k_fold_idx} training')\n",
    "  ax.plot(history.history['val_loss'], color=color, ls='--', label=f'{k_fold_idx} val')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('loss')\n",
    "ax.legend()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80add66e",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Calculate the mean and std of the validation loss.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad5ca84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27a38154",
   "metadata": {},
   "source": [
    "## Save and Load a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9d6d78",
   "metadata": {},
   "source": [
    "As you have seen training a model takes some time. So retrain a model each time you need it is definitely not the way to go. Instead one should save the model after the training and load it for application.\n",
    "\n",
    "Saving and loading a model is very straight forward with:<br>\n",
    "Use `model.save('path/to/location', save_format='h5')` to save a model <br>\n",
    "and `model = tf.keras.models.load_model('path/to/location')` to load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907eab0a",
   "metadata": {},
   "source": [
    "Lets save the three models of the cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5145262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all models\n",
    "for idx, model in enumerate(kfold_model):\n",
    "    # Save the model\n",
    "    model.save(f'models/test_crossval_model_{idx}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bdf5d5",
   "metadata": {},
   "source": [
    "Now load one of the models and plot its summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee95e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('models/test_crossval_model_0')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96682fff",
   "metadata": {},
   "source": [
    "## Train your own Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81022096",
   "metadata": {},
   "source": [
    "Now it is your turn to create your own model to hunt for the Higgs boson. For the beginning, we will have a look on a baseline model. This baseline model does already a good job in the classification and it is up to you to create a better one.\n",
    "\n",
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "1. Use reweighted event weights for your training\n",
    "2. Choose a setup for your model\n",
    "3. Train your model with early stopping\n",
    "4. Plot the training history and binary classification on training and validation data\n",
    "5. check your validation loss <br>\n",
    "    If the validation loss is not better than for the baseline model by two standard deviations of the baseline validation loss: go back to step 2.\n",
    "6. Validate your results with cross validation and calculate the mean validation loss and its standard deviation\n",
    "    If the loss is not significantly better than for the baseline model: go back to step 2.\n",
    "7. Save your models and training plots\n",
    "8. Plot the training history\n",
    "\n",
    "Document what you can observe for your own model.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e74c85",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2301af",
   "metadata": {},
   "source": [
    "Lets load the baseline models. The baseline model was trained with cross validation on 40% of the data for training and 20% of the total data for validation.\n",
    "\n",
    "The val loss of the model is $0.27785 \\pm 0.01361$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f312de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "baseline_models = []\n",
    "for idx in range(3):\n",
    "    print(f'baseline_models/crossval_model_{idx}')\n",
    "    model = tf.keras.models.load_model(f'baseline_models/crossval_model_{idx}')\n",
    "    baseline_models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d92e9d",
   "metadata": {},
   "source": [
    "### Training input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9f74ea",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Train your model for all processes on all <u><b>low level</b></u> variables.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18931060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use all processes\n",
    "sample_list_signal = ['ggH125_ZZ4lep', 'VBFH125_ZZ4lep', 'WH125_ZZ4lep', 'ZH125_ZZ4lep']\n",
    "sample_list_background = ['llll', 'Zee', 'Zmumu', 'ttbar_lep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b74d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training input variables\n",
    "training_variables = ['lep1_pt', 'lep2_pt', 'lep3_pt', 'lep4_pt']\n",
    "training_variables += ['lep1_e', 'lep2_e', 'lep3_e', 'lep4_e']\n",
    "training_variables += ['lep1_charge', 'lep2_charge', 'lep3_charge', 'lep4_charge']\n",
    "training_variables += ['lep1_pdgId', 'lep2_pdgId', 'lep3_pdgId', 'lep4_pdgId']\n",
    "training_variables += ['lep1_phi', 'lep2_phi', 'lep3_phi', 'lep4_phi']\n",
    "training_variables += ['lep1_eta', 'lep2_eta', 'lep3_eta', 'lep4_eta']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73af8236",
   "metadata": {},
   "source": [
    "Use again 40% of the data for training and 20% for validation. If you want to improve your training on a later point you can also use 80% for training and 20% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a69876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "values, weights, classification = common.get_dnn_input(data_frames, training_variables, sample_list_signal, sample_list_background, 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d0353d",
   "metadata": {},
   "source": [
    "If you want you can play around with the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22472da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get reweighted weights\n",
    "train_weights_reweighted = reweight_weights(train_weights, train_classification)\n",
    "val_weights_reweighted = reweight_weights(val_weights, val_classification)\n",
    "\n",
    "# Convert the data to tensorflow datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcb6be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9292287",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffc577f",
   "metadata": {},
   "source": [
    "You can change the learning rate of the optimizer to improve your training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6751bf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8813852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57638d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc12e39",
   "metadata": {},
   "source": [
    "Create your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af1c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization layer\n",
    "\n",
    "# Create a simple NN\n",
    "model_layers = [\n",
    "]\n",
    "model = tf.keras.models.Sequential(model_layers)\n",
    "# Compile model\n",
    "model.compile(optimizer=adam_optimizer, loss=loss_fn, weighted_metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f8e57d",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e31e56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f4eb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a9d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on training and validation data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5137f52",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83905241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3432957b",
   "metadata": {
    "id": "3432957b"
   },
   "outputs": [],
   "source": [
    "# Store the models and their training history\n",
    "kfold_history = []\n",
    "kfold_model = []\n",
    "# Store the evaluation on training and validation data\n",
    "kfold_train_eval_loss = []\n",
    "kfold_train_eval_acc = []\n",
    "kfold_val_eval_loss = []\n",
    "kfold_val_eval_acc = []\n",
    "split_idx = 1\n",
    "for train_indices, val_indices in kfold.split(values):\n",
    "    print(f'Use fold {split_idx}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476f3a59",
   "metadata": {
    "id": "476f3a59"
   },
   "outputs": [],
   "source": [
    "# Loop over all models\n",
    "for idx, model in enumerate(kfold_model):\n",
    "    # Save the model\n",
    "    model.save(f'models/own_model_crossval_{idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1b2774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47d4d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the prediction on training and validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b7f7e9",
   "metadata": {},
   "source": [
    "## Apply your Model for Higgs search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be269889",
   "metadata": {},
   "source": [
    "So lets use the model to measure the Higgs boson. The model prediction can be used to suppress background events.\n",
    "With a selection criteria we can increase the signal purity and improve the Higgs significance.\n",
    "The statistic significance of the Higgs boson can be estimated by:\n",
    "\n",
    "$Z_{stat} = \\frac{N_{Higgs}}{\\sqrt{N_{bkg}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eca8432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the values, weights, and classification for the full dataset\n",
    "values, weights, classification = common.get_dnn_input(data_frames, training_variables, sample_list_signal, sample_list_background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792b55e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data in signal and background\n",
    "signal_values = values[classification > 0.5]\n",
    "signal_weights = weights[classification > 0.5]\n",
    "bkg_values = values[classification < 0.5]\n",
    "bkg_weights = weights[classification < 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6701b98",
   "metadata": {},
   "source": [
    "Lets derive the significance with the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793ee479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model prediction\n",
    "signal_prediction = baseline_models[0].predict(signal_values)\n",
    "bkg_prediction = baseline_models[0].predict(bkg_values)\n",
    "# Transform predicton to array\n",
    "signal_prediction = np.array([element[0] for element in signal_prediction])\n",
    "bkg_prediction = np.array([element[0] for element in bkg_prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2832c67d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cut_value = 0.\n",
    "# Number of signal and background events passing the prediction selection\n",
    "n_signal = signal_weights[signal_prediction > cut_value].sum()\n",
    "n_bkg = bkg_weights[bkg_prediction > cut_value].sum()\n",
    "\n",
    "# Significance\n",
    "significance = n_signal / np.sqrt(n_bkg)\n",
    "\n",
    "print(f'The prediction selection is passed by {round(n_signal, 2)} signal and {round(n_bkg, 2)} background events.')\n",
    "print(f'This results in a significance of {round(significance, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a4369c",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "Calculate and plot the significances for different cuts on the prediction value. Do the calculation in a for loop and break if the number of background events is lower than 10\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c75c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_values = []\n",
    "significances = []\n",
    "for cut_value in np.linspace(0, 1, 1000):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05168abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the significances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790e4a6f",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "What is the maximal significance you got for the model? What would be the best possible significance?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c12c2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The best significance by the given model is {round(max(significances), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8737e915",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "What is the best significance of your model? Plot the significances dependend on the cut on the prediction value.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8495b83",
   "metadata": {},
   "source": [
    "## Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e2d1f7",
   "metadata": {},
   "source": [
    "The jupyter notebook is still very work in progress. To improve this machine learning course your feedback is needed.\n",
    "\n",
    "<font color='blue'>\n",
    "Task:\n",
    "\n",
    "What were good aspects of this notebook? What should have been more detailed and what was too much? Do you have any other suggestions for improvement?<br>\n",
    "Please be honest but stay polite :)\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd96e9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
